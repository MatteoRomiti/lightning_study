{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linking Heuristics\n",
    "In this notebook, we present two linking heuristics and we compare the results with our ground truth data. The sections are:\n",
    "1. Linking Heuristic 1\n",
    "    - 1.1 Find Reused Coins\n",
    "    - 1.2 Linking\n",
    "2. Linking Heuristic 2\n",
    "    - Linking\n",
    "3. Validation\n",
    "    - 3.1 Prepare Ground Truth\n",
    "    - 3.2 Compare with Ground Truth\n",
    "    - 3.3 Compare with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:47:50.871311Z",
     "start_time": "2020-10-01T15:47:50.858745Z"
    }
   },
   "outputs": [],
   "source": [
    "from api_calls import get_address_txs\n",
    "from utils import read_json, write_json, on_chain_heuristics_list, set_mapping, get_results, most_common, \\\n",
    "    link_other_nodes, invert_mapping, add_node_to_entity, get_entity_neighbors, df_to_dicts_set\n",
    "\n",
    "# input files\n",
    "from utils import funded_address_settlement_txs_file, funding_address_entity_file, settlement_address_entity_file, \\\n",
    "    channels_file, funding_txs_file, settlement_addresses_file, settlement_txs_file, outgoing_channels_file, \\\n",
    "    incoming_channels_file\n",
    "\n",
    "# outputs files\n",
    "from utils import funding_entity_channels_nodes_file, heuristics_files, gt_node_entity_file, gt_address_txs_file, \\\n",
    "    entity_nbrs_file, nodes_csv_file\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linking heuristic 1\n",
    "In this section, we first read and prepare some data and then we perform the linking heuristic 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:53:35.540882Z",
     "start_time": "2020-10-01T15:53:29.958090Z"
    }
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "channels_df = pd.read_csv(channels_file)\n",
    "funding_address_entity = read_json(funding_address_entity_file)\n",
    "settlement_address_entity = read_json(settlement_address_entity_file)\n",
    "funding_txs = read_json(funding_txs_file)\n",
    "settlement_addresses = set(read_json(settlement_addresses_file))\n",
    "funded_address_settlement_txs = read_json(funded_address_settlement_txs_file)\n",
    "settlement_txs = read_json(settlement_txs_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:53:35.741275Z",
     "start_time": "2020-10-01T15:53:35.542916Z"
    }
   },
   "outputs": [],
   "source": [
    "# given a node, tell me its channels\n",
    "node_channels = dict()\n",
    "for channel in channels_df.values:\n",
    "    c, n1, n2 = channel\n",
    "    if n1 not in node_channels:\n",
    "        node_channels[n1] = set()\n",
    "    node_channels[n1].add(c)\n",
    "    if n2 not in node_channels:\n",
    "        node_channels[n2] = set()\n",
    "    node_channels[n2].add(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:53:36.613128Z",
     "start_time": "2020-10-01T15:53:35.743055Z"
    }
   },
   "outputs": [],
   "source": [
    "# Nodes on-chain activity\n",
    "# for each node, create a list of timestamps of\n",
    "# openings, closings and first/last_activity\n",
    "node_openings_closings = dict()\n",
    "for node, chnls in node_channels.items():\n",
    "    node_openings_closings[node] = {'openings': [], 'closings': []}\n",
    "    for chnl in chnls:\n",
    "        tx_hsh, out_index = chnl.split(':')\n",
    "        t_open = funding_txs[tx_hsh]['status']['block_time']\n",
    "        node_openings_closings[node]['openings'].append(t_open)\n",
    "\n",
    "        t_closed = 0\n",
    "        funded_address = funding_txs[tx_hsh]['vout'][int(out_index)]['scriptpubkey_address']\n",
    "        stxs = funded_address_settlement_txs[funded_address]\n",
    "        if stxs:\n",
    "            t_closed = stxs[0]['status']['block_time']\n",
    "        node_openings_closings[node]['closings'].append(t_closed)\n",
    "    node_openings_closings[node]['first_activity'] = min(\n",
    "        node_openings_closings[node]['openings'])\n",
    "    node_openings_closings[node]['last_activity'] = max(\n",
    "        max(node_openings_closings[node]['openings']),\n",
    "        max(node_openings_closings[node]['closings']))\n",
    "    if min(node_openings_closings[node]['closings']) == 0:\n",
    "        # still open -> now\n",
    "        node_openings_closings[node]['last_activity'] = int(time.time())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Find channels reusing coins from other channels\n",
    "Here we look for channels that were funded with settlement coins (outputs of settlement txs of other channels). We also use the on-chain clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:53:44.436221Z",
     "start_time": "2020-10-01T15:53:36.615379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use stars\n",
      "use snakes\n",
      "use collectors\n",
      "use proxies\n"
     ]
    }
   ],
   "source": [
    "# use all on-chain clustering heuristics to have a wider overlap\n",
    "# then the linking heuristics will decide which triplets to use\n",
    "och = {h: (True if h != 'none' else False) for h in on_chain_heuristics_list}\n",
    "fae, sae, = set_mapping(funding_address_entity, settlement_address_entity, och)\n",
    "\n",
    "fes = set(fae.values())\n",
    "ses = set(sae.values())\n",
    "overlap_entities = fes.intersection(ses)\n",
    "\n",
    "chpoints_reusing_coins = set()\n",
    "settlement_entities = sae.values()\n",
    "for chpoint in channels_df.chan_point.values:\n",
    "    hsh, out_index = chpoint.split(':')\n",
    "    ftx = funding_txs[hsh]\n",
    "    for inp in ftx['vin']:\n",
    "        e = fae[inp['prevout']['scriptpubkey_address']]\n",
    "        if e in overlap_entities and e in settlement_entities:\n",
    "            chpoints_reusing_coins.add(chpoint)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Linking\n",
    "Here is the actual linking heuristic that we run using different on-chain patterns separately and then all of them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:53:51.739877Z",
     "start_time": "2020-10-01T15:53:51.710323Z"
    }
   },
   "outputs": [],
   "source": [
    "use_entities = True\n",
    "\n",
    "def heuristic_1(fae, sae, och, files):\n",
    "    print()\n",
    "    # create a copy of initial state\n",
    "    funding_address_entity = {k: v for k, v in fae.items()}\n",
    "    settlement_address_entity = {k: v for k, v in sae.items()}\n",
    "\n",
    "    # prepare results\n",
    "    r = dict()\n",
    "    r['n_funding_entities'] = len(set(funding_address_entity.values()))\n",
    "    r['n_settlement_entities'] = len(set(settlement_address_entity.values()))\n",
    "    r['n_entities'] = len(set(settlement_address_entity.values()).union(set(funding_address_entity.values())))\n",
    "    r['n_addresses'] = len(set(settlement_address_entity.keys()).union(set(funding_address_entity.keys())))\n",
    "    r['n_nodes'] = len(node_channels)\n",
    "\n",
    "    # map entities to components\n",
    "    funding_address_entity, settlement_address_entity = \\\n",
    "        set_mapping(funding_address_entity, settlement_address_entity, och)\n",
    "\n",
    "    # print('Start heuristic 1...')\n",
    "\n",
    "#     print('use_entities', use_entities)\n",
    "\n",
    "    # # mapping between stx and its ftx\n",
    "    stx_its_chpoint = dict()\n",
    "    for channel in channels_df.values:\n",
    "        funding_tx, out_index = channel[0].split(':')\n",
    "        funded_address = funding_txs[funding_tx]['vout'][int(out_index)]['scriptpubkey_address']\n",
    "        settlement_txs_fa = funded_address_settlement_txs[funded_address]\n",
    "        if len(settlement_txs_fa) == 1:  # it is always zero or one tx\n",
    "            stx = settlement_txs_fa[0]['txid']\n",
    "            if stx not in stx_its_chpoint:\n",
    "                stx_its_chpoint[stx] = channel[0]\n",
    "            else:\n",
    "                print('stx already in dict', stx)\n",
    "\n",
    "    # create links for heuristic 1 (both at address and entity level)\n",
    "    stx_a_chpoint = []  # list of settlement tx, address, funding tx\n",
    "    for chpoint in chpoints_reusing_coins:\n",
    "        hsh, out_index = chpoint.split(':')\n",
    "        uftx = funding_txs[hsh]\n",
    "        for i in uftx['vin']:\n",
    "            a = i['prevout']['scriptpubkey_address']\n",
    "            prev_tx = i['txid']\n",
    "            if a in settlement_addresses:\n",
    "                if prev_tx in settlement_txs:\n",
    "                    stx_a_chpoint.append([prev_tx, a, chpoint])\n",
    "    #             else:\n",
    "    #                 # a is a settlement_address but prev_tx is not a\n",
    "    #                 settlement_tx in our data\n",
    "\n",
    "    stx_e_chpoint = []  # list of settlement tx, entity, chpoint\n",
    "    print('n coins reused', len(chpoints_reusing_coins))\n",
    "    settlement_entities = set(settlement_address_entity.values())\n",
    "    for chpoint in chpoints_reusing_coins:\n",
    "        hsh, out_index = chpoint.split(':')\n",
    "        uftx = funding_txs[hsh]\n",
    "        for i in uftx['vin']:\n",
    "            e = funding_address_entity[i['prevout']['scriptpubkey_address']]\n",
    "            prev_tx = i['txid']\n",
    "            if e in settlement_entities:\n",
    "                if prev_tx in settlement_txs:\n",
    "                    stx_e_chpoint.append([prev_tx, e, chpoint])\n",
    "\n",
    "    # I need a mapping between ch_point and nodes\n",
    "    # and between settlement tx and nodes\n",
    "    chpoint_nodes = dict()\n",
    "    for channel in channels_df.values:\n",
    "        chpoint_nodes[channel[0]] = [channel[1], channel[2]]\n",
    "\n",
    "    funded_address_chpoint = dict()\n",
    "    for chpoint in channels_df.chan_point.values:\n",
    "        hsh, out_index = chpoint.split(':')\n",
    "        funded_address = funding_txs[hsh]['vout'][int(out_index)]['scriptpubkey_address']\n",
    "        if funded_address not in funded_address_chpoint:\n",
    "            funded_address_chpoint[funded_address] = chpoint\n",
    "        else:\n",
    "            print(funded_address, ' has multiple channels')\n",
    "\n",
    "    stx_nodes = dict()\n",
    "    for fa, chpoint in funded_address_chpoint.items():\n",
    "        stxs = funded_address_settlement_txs[fa]\n",
    "        if stxs:\n",
    "            stx = stxs[0]['txid']\n",
    "            stx_nodes[stx] = chpoint_nodes[chpoint]\n",
    "    # print('Initial number of links addresses', len(stx_a_ftx))\n",
    "    # print('Initial number of links entities', len(stx_e_ftx))\n",
    "\n",
    "    # decide link level\n",
    "    triplet = stx_a_chpoint\n",
    "    if use_entities:\n",
    "        triplet = stx_e_chpoint\n",
    "\n",
    "    links = []  # like stx_a_chpoint plus 4 nodes of channels\n",
    "    for el in triplet:\n",
    "        # the funding entity controls the node in common between the channel\n",
    "        # opened with ftx and closed with stx\n",
    "        stx, a, chpoint = el\n",
    "        n1, n2 = chpoint_nodes[chpoint]  # happens after the stx\n",
    "        n3, n4 = stx_nodes[stx]\n",
    "        links.append([stx, a, chpoint, n1, n2, n3, n4])\n",
    "\n",
    "    useful_links = []\n",
    "    for link in links:\n",
    "        s = set(link[3:])\n",
    "        if len(s) == 3:\n",
    "            useful_links.append(link)\n",
    "\n",
    "    # if closing of other node in ch1 > opening of other node in ch2\n",
    "    # then we can use the link\n",
    "    usable_links = []\n",
    "    for link in useful_links:\n",
    "        node_in_common = most_common(link[3:])\n",
    "        other_node_ch1 = ''\n",
    "        other_node_ch2 = ''\n",
    "        for node in link[3:][::-1]:\n",
    "            if node != node_in_common:\n",
    "                if not other_node_ch1:\n",
    "                    other_node_ch1 = node\n",
    "                else:\n",
    "                    other_node_ch2 = node\n",
    "        if node_openings_closings[other_node_ch1]['last_activity'] > \\\n",
    "                node_openings_closings[other_node_ch2]['first_activity']:\n",
    "            usable_links.append(link)\n",
    "\n",
    "    reliable_links_addresses = []\n",
    "    for link in usable_links:\n",
    "        link_address = link[1]\n",
    "        stx = link[0]\n",
    "        its_ftx = stx_its_chpoint[stx].split(':')[0]\n",
    "        if link_address in [el['prevout']['scriptpubkey_address'] for el in\n",
    "                            funding_txs[its_ftx]['vin']]:\n",
    "            reliable_links_addresses.append(link)\n",
    "    print('Number of reliable links at address level:',\n",
    "          len(reliable_links_addresses))\n",
    "\n",
    "    reliable_links_entities = []\n",
    "    entities_reusing = set()\n",
    "    for link in usable_links:\n",
    "        if use_entities:\n",
    "            link_entity = link[1]\n",
    "        else:\n",
    "            link_entity = settlement_address_entity[link[1]]\n",
    "        stx = link[0]\n",
    "        its_ftx = stx_its_chpoint[stx].split(':')[0]\n",
    "        if link_entity in [funding_address_entity[el['prevout']['scriptpubkey_address']] for el\n",
    "                           in funding_txs[its_ftx]['vin']]:\n",
    "            entities_reusing.add(link_entity)\n",
    "            reliable_links_entities.append(link)\n",
    "\n",
    "    print('Number of reliable links at entity level:', len(reliable_links_entities))\n",
    "    print('Number of entities reusing funding addresses:', len(entities_reusing))\n",
    "\n",
    "    # step 1: linking nodes to entity using stx and ftx\n",
    "    # print('Step 1:')\n",
    "    heuristic_1a_entity_node = dict()\n",
    "    heuristic_1a_node_entity = dict()\n",
    "    for link in reliable_links_entities:\n",
    "        if use_entities:\n",
    "            e = link[1]\n",
    "        else:\n",
    "            e = settlement_address_entity[link[1]]\n",
    "        n = most_common(link[3:])\n",
    "        if e not in heuristic_1a_entity_node:\n",
    "            heuristic_1a_entity_node[e] = set()\n",
    "        heuristic_1a_entity_node[e].add(n)\n",
    "        if n not in heuristic_1a_node_entity:\n",
    "            heuristic_1a_node_entity[n] = set()\n",
    "        heuristic_1a_node_entity[n].add(e)\n",
    "    # print('Number of entities linked to nodes:', len(heuristic_1a_entity_node))\n",
    "    # print('Number of nodes linked to entities:', len(heuristic_1a_node_entity))\n",
    "\n",
    "    # print('Step 2:')\n",
    "    # link other node and entity in channel\n",
    "    heuristic_1b_entity_node = link_other_nodes(heuristic_1a_entity_node, channels_df,\n",
    "                                                funded_address_settlement_txs,\n",
    "                                                funding_txs,\n",
    "                                                settlement_address_entity)\n",
    "    heuristic_1b_node_entity = invert_mapping(heuristic_1b_entity_node)\n",
    "\n",
    "    # correct means that the settlement tx has exactly two output entities\n",
    "    correct_stxs = []  # correct stxs\n",
    "    correct_settlement_entities = set()  # output entities of correct stxs\n",
    "    correct_nodes = set()\n",
    "    for channel in channels_df.values:\n",
    "        funding_tx, out_index = channel[0].split(':')\n",
    "        node_1 = channel[1]\n",
    "        node_2 = channel[2]\n",
    "        funded_address = \\\n",
    "            funding_txs[funding_tx]['vout'][int(out_index)]['scriptpubkey_address']\n",
    "\n",
    "        settlement_txs_fa = funded_address_settlement_txs[funded_address]\n",
    "        # if channel is closed and number of outputs == 2 and\n",
    "        # one node is mapped to one entity in the outputs\n",
    "        if settlement_txs_fa:  # it is always only one\n",
    "            for settlement_tx in settlement_txs_fa:\n",
    "                # count entities\n",
    "                entities = set([settlement_address_entity[out['scriptpubkey_address']]\n",
    "                                for out in settlement_tx['vout']])\n",
    "                if len(entities) == 2:\n",
    "                    correct_stxs.append(settlement_tx)\n",
    "                    correct_settlement_entities = correct_settlement_entities.union(entities)\n",
    "                    correct_nodes.add(node_1)\n",
    "                    correct_nodes.add(node_2)\n",
    "\n",
    "    perc_entities_linked_settled = round(100 * len(heuristic_1b_entity_node) / r['n_settlement_entities'], 2)\n",
    "    perc_entities_linked_2e = round(100 * len(heuristic_1b_entity_node) / len(correct_settlement_entities), 2)\n",
    "    perc_nodes_linked_2e = round(100 * len(heuristic_1b_node_entity) / len(correct_nodes), 2)\n",
    "\n",
    "    r = get_results(r, heuristic_1b_entity_node, heuristic_1b_node_entity)\n",
    "\n",
    "    print('Number of settlement entities:', r['n_settlement_entities'], '--', perc_entities_linked_settled, '% linked')\n",
    "    print('Number of settlement entities considering settlement txs with 2 output entities:', len(correct_settlement_entities), '--', perc_entities_linked_2e, '% linked')\n",
    "    print('Number of nodes considering settlement txs with 2 output entities:', len(correct_nodes), '--', perc_nodes_linked_2e, '% linked')\n",
    "\n",
    "    addresses_linked = set()\n",
    "    for address_entity in [funding_address_entity, settlement_address_entity]:\n",
    "        for address, entity in address_entity.items():\n",
    "            if entity in heuristic_1b_entity_node:\n",
    "                addresses_linked.add(address)\n",
    "    r['perc_addresses_linked'] = round(\n",
    "        100 * len(addresses_linked) / r['n_addresses'], 2)\n",
    "\n",
    "    output_file_a, output_file_b = files[1]['all']\n",
    "    for k in ['stars', 'none', 'snakes', 'collectors', 'proxies', 'all']:\n",
    "        if och[k]:\n",
    "            output_file_a, output_file_b = files[1][k]\n",
    "\n",
    "    # Write to file\n",
    "    heuristic_1_entity_node = {str(k): [e for e in v]\n",
    "                               for k, v in heuristic_1b_entity_node.items()}\n",
    "    heuristic_1_node_entity = {k: [int(e) for e in v]\n",
    "                               for k, v in heuristic_1b_node_entity.items()}\n",
    "    print('On-chain clustering', och)\n",
    "    print('writing to', output_file_a, output_file_b)\n",
    "    write_json(heuristic_1_entity_node, output_file_a)\n",
    "    write_json(heuristic_1_node_entity, output_file_b)\n",
    "\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:55:04.444573Z",
     "start_time": "2020-10-01T15:53:53.616731Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n coins reused 12149\n",
      "Number of reliable links at address level: 0\n",
      "Number of reliable links at entity level: 83\n",
      "Number of entities reusing funding addresses: 22\n",
      "Iteration: 1 -- Number of linked entities: 22\n",
      "Iteration: 2 -- Number of linked entities: 3351\n",
      "Iteration: 3 -- Number of linked entities: 7403\n",
      "Iteration: 4 -- Number of linked entities: 8645\n",
      "Iteration: 5 -- Number of linked entities: 8945\n",
      "Iteration: 6 -- Number of linked entities: 9027\n",
      "Iteration: 7 -- Number of linked entities: 9042\n",
      "Number of settlement entities: 53370 -- 16.94 % linked\n",
      "Number of settlement entities considering settlement txs with 2 output entities: 32321 -- 27.98 % linked\n",
      "Number of nodes considering settlement txs with 2 output entities: 4626 -- 46.91 % linked\n",
      "On-chain clustering {'none': True, 'stars': False, 'snakes': False, 'collectors': False, 'proxies': False, 'all': False}\n",
      "writing to ../data/results/none_1_entity_node.json ../data/results/none_1_node_entity.json\n",
      "\n",
      "use stars\n",
      "n coins reused 12149\n",
      "Number of reliable links at address level: 0\n",
      "Number of reliable links at entity level: 83\n",
      "Number of entities reusing funding addresses: 22\n",
      "Iteration: 1 -- Number of linked entities: 22\n",
      "Iteration: 2 -- Number of linked entities: 3351\n",
      "Iteration: 3 -- Number of linked entities: 7403\n",
      "Iteration: 4 -- Number of linked entities: 8645\n",
      "Iteration: 5 -- Number of linked entities: 8945\n",
      "Iteration: 6 -- Number of linked entities: 9027\n",
      "Iteration: 7 -- Number of linked entities: 9042\n",
      "Number of settlement entities: 53370 -- 16.94 % linked\n",
      "Number of settlement entities considering settlement txs with 2 output entities: 32315 -- 27.98 % linked\n",
      "Number of nodes considering settlement txs with 2 output entities: 4626 -- 46.91 % linked\n",
      "On-chain clustering {'none': False, 'stars': True, 'snakes': False, 'collectors': False, 'proxies': False, 'all': False}\n",
      "writing to ../data/results/stars_1_entity_node.json ../data/results/stars_1_node_entity.json\n",
      "\n",
      "use snakes\n",
      "n coins reused 12149\n",
      "Number of reliable links at address level: 0\n",
      "Number of reliable links at entity level: 83\n",
      "Number of entities reusing funding addresses: 22\n",
      "Iteration: 1 -- Number of linked entities: 22\n",
      "Iteration: 2 -- Number of linked entities: 3351\n",
      "Iteration: 3 -- Number of linked entities: 7403\n",
      "Iteration: 4 -- Number of linked entities: 8645\n",
      "Iteration: 5 -- Number of linked entities: 8945\n",
      "Iteration: 6 -- Number of linked entities: 9027\n",
      "Iteration: 7 -- Number of linked entities: 9042\n",
      "Number of settlement entities: 53370 -- 16.94 % linked\n",
      "Number of settlement entities considering settlement txs with 2 output entities: 32321 -- 27.98 % linked\n",
      "Number of nodes considering settlement txs with 2 output entities: 4626 -- 46.91 % linked\n",
      "On-chain clustering {'none': False, 'stars': False, 'snakes': True, 'collectors': False, 'proxies': False, 'all': False}\n",
      "writing to ../data/results/snakes_1_entity_node.json ../data/results/snakes_1_node_entity.json\n",
      "\n",
      "use collectors\n",
      "n coins reused 12149\n",
      "Number of reliable links at address level: 0\n",
      "Number of reliable links at entity level: 83\n",
      "Number of entities reusing funding addresses: 22\n",
      "Iteration: 1 -- Number of linked entities: 22\n",
      "Iteration: 2 -- Number of linked entities: 3341\n",
      "Iteration: 3 -- Number of linked entities: 7446\n",
      "Iteration: 4 -- Number of linked entities: 8763\n",
      "Iteration: 5 -- Number of linked entities: 9099\n",
      "Iteration: 6 -- Number of linked entities: 9180\n",
      "Iteration: 7 -- Number of linked entities: 9193\n",
      "Number of settlement entities: 53370 -- 17.23 % linked\n",
      "Number of settlement entities considering settlement txs with 2 output entities: 31084 -- 29.57 % linked\n",
      "Number of nodes considering settlement txs with 2 output entities: 4620 -- 47.16 % linked\n",
      "On-chain clustering {'none': False, 'stars': False, 'snakes': False, 'collectors': True, 'proxies': False, 'all': False}\n",
      "writing to ../data/results/collectors_1_entity_node.json ../data/results/collectors_1_node_entity.json\n",
      "\n",
      "use proxies\n",
      "n coins reused 12149\n",
      "Number of reliable links at address level: 0\n",
      "Number of reliable links at entity level: 83\n",
      "Number of entities reusing funding addresses: 22\n",
      "Iteration: 1 -- Number of linked entities: 22\n",
      "Iteration: 2 -- Number of linked entities: 3689\n",
      "Iteration: 3 -- Number of linked entities: 8497\n",
      "Iteration: 4 -- Number of linked entities: 10037\n",
      "Iteration: 5 -- Number of linked entities: 10434\n",
      "Iteration: 6 -- Number of linked entities: 10569\n",
      "Iteration: 7 -- Number of linked entities: 10584\n",
      "Number of settlement entities: 53370 -- 19.83 % linked\n",
      "Number of settlement entities considering settlement txs with 2 output entities: 31712 -- 33.38 % linked\n",
      "Number of nodes considering settlement txs with 2 output entities: 4628 -- 53.78 % linked\n",
      "On-chain clustering {'none': False, 'stars': False, 'snakes': False, 'collectors': False, 'proxies': True, 'all': False}\n",
      "writing to ../data/results/proxies_1_entity_node.json ../data/results/proxies_1_node_entity.json\n",
      "\n",
      "use stars\n",
      "use snakes\n",
      "use collectors\n",
      "use proxies\n",
      "n coins reused 12149\n",
      "Number of reliable links at address level: 0\n",
      "Number of reliable links at entity level: 83\n",
      "Number of entities reusing funding addresses: 22\n",
      "Iteration: 1 -- Number of linked entities: 22\n",
      "Iteration: 2 -- Number of linked entities: 3726\n",
      "Iteration: 3 -- Number of linked entities: 8804\n",
      "Iteration: 4 -- Number of linked entities: 10627\n",
      "Iteration: 5 -- Number of linked entities: 11141\n",
      "Iteration: 6 -- Number of linked entities: 11257\n",
      "Iteration: 7 -- Number of linked entities: 11272\n",
      "Number of settlement entities: 53370 -- 21.12 % linked\n",
      "Number of settlement entities considering settlement txs with 2 output entities: 30440 -- 37.03 % linked\n",
      "Number of nodes considering settlement txs with 2 output entities: 4621 -- 55.81 % linked\n",
      "On-chain clustering {'none': False, 'stars': True, 'snakes': True, 'collectors': True, 'proxies': True, 'all': True}\n",
      "writing to ../data/results/all_1_entity_node.json ../data/results/all_1_node_entity.json\n"
     ]
    }
   ],
   "source": [
    "on_chain_heuristics = {och: False for och in on_chain_heuristics_list}\n",
    "\n",
    "results_1 = dict()\n",
    "for och in on_chain_heuristics:\n",
    "    # one by one\n",
    "    if och != 'all':\n",
    "        on_chain_heuristics[och] = True\n",
    "        results_1[och] = heuristic_1(funding_address_entity, settlement_address_entity, on_chain_heuristics, heuristics_files)\n",
    "        on_chain_heuristics[och] = False\n",
    "\n",
    "# all\n",
    "on_chain_heuristics = {och: (True if och != 'none' else False) for och in on_chain_heuristics_list}\n",
    "results_1['all'] = heuristic_1(funding_address_entity, settlement_address_entity, on_chain_heuristics, heuristics_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T12:23:27.102888Z",
     "start_time": "2020-10-01T12:23:27.098588Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_funding_entities': 96181,\n",
       " 'n_settlement_entities': 53370,\n",
       " 'n_entities': 138457,\n",
       " 'n_addresses': 238070,\n",
       " 'n_nodes': 10910,\n",
       " 'n_entities_linked': 11272,\n",
       " 'n_nodes_linked': 2579,\n",
       " 'perc_entities_linked': 8.14,\n",
       " 'perc_nodes_linked': 23.64,\n",
       " 'perc_addresses_linked': 20.96}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json(results, heuristics_files[1]['results'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T20:18:45.035685Z",
     "start_time": "2020-09-28T20:18:45.032655Z"
    }
   },
   "source": [
    "## 2. Linking heuristic 2\n",
    "Here we run the linking heuristic 2 using on-chain clustering separately and then combined together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:17:56.938045Z",
     "start_time": "2020-10-01T15:17:56.920219Z"
    }
   },
   "outputs": [],
   "source": [
    "def heuristic_2(fae, sae, och, files):\n",
    "    print()\n",
    "    min_conf = 2  # min confidence level for results\n",
    "\n",
    "    funding_address_entity = {k: v for k, v in fae.items()}\n",
    "    settlement_address_entity = {k: v for k, v in sae.items()}\n",
    "    r = dict()\n",
    "    r['n_funding_entities'] = len(set(funding_address_entity.values()))\n",
    "    r['n_settlement_entities'] = len(set(settlement_address_entity.values()))\n",
    "    r['n_entities'] = len(set(settlement_address_entity.values()).union(set(funding_address_entity.values())))\n",
    "    r['n_addresses'] = len(set(settlement_address_entity.keys()).union(set(funding_address_entity.keys())))\n",
    "    r['n_nodes'] = len(node_channels)\n",
    "\n",
    "    funding_address_entity, settlement_address_entity, = \\\n",
    "        set_mapping(funding_address_entity, settlement_address_entity, och)\n",
    "\n",
    "    # print('Start heuristic 2...')\n",
    "    # print('Step 1:')\n",
    "    funding_entity_possible_nodes = dict()\n",
    "    for channel in channels_df.values:\n",
    "        funding_tx, out_index = channel[0].split(':')\n",
    "        funding_address = funding_txs[funding_tx]['vin'][0]['prevout']['scriptpubkey_address']\n",
    "        funding_entity = funding_address_entity[funding_address]\n",
    "        if funding_entity not in funding_entity_possible_nodes:\n",
    "            funding_entity_possible_nodes[funding_entity] = []\n",
    "        funding_entity_possible_nodes[funding_entity].append(channel[1])\n",
    "        funding_entity_possible_nodes[funding_entity].append(channel[2])\n",
    "\n",
    "    # each funding entity that has at least n_channels possible nodes\n",
    "    # (confidence level >= n_channels)\n",
    "    n_channels = min_conf\n",
    "    entity_channels_half = []\n",
    "    fe_confidence = []\n",
    "    fe_confidence_dict = dict()\n",
    "    for fe, pns in funding_entity_possible_nodes.items():\n",
    "        if len(pns) >= n_channels * 2:  # *2 cause we have two nodes per channel\n",
    "            pn_occur = Counter(pns)\n",
    "            for pn, occur in pn_occur.items():\n",
    "                if occur * 2 == len(pns):\n",
    "                    fe_confidence.append([fe, occur])\n",
    "                    fe_confidence_dict[fe] = occur\n",
    "                    entity_channels_half.append(occur)\n",
    "    entity_channels_half.sort()\n",
    "\n",
    "    funding_entity_channels_nodes = dict()\n",
    "    node_possible_entities = dict()\n",
    "    # populate funding_entity_channels_nodes\n",
    "    for channel in channels_df.values:\n",
    "        funding_tx, out_index = channel[0].split(':')\n",
    "        funding_address = funding_txs[funding_tx]['vin'][0]['prevout']['scriptpubkey_address']\n",
    "        funding_entity = funding_address_entity[funding_address]\n",
    "        if funding_entity not in funding_entity_channels_nodes:\n",
    "            # use chan_point as key\n",
    "            funding_entity_channels_nodes[funding_entity] = dict()\n",
    "        # add nodes\n",
    "        funding_entity_channels_nodes[funding_entity][channel[0]] = [channel[1],\n",
    "                                                                     channel[2]]\n",
    "        for i in [1, 2]:\n",
    "            if channel[i] not in node_possible_entities:\n",
    "                node_possible_entities[channel[i]] = set()\n",
    "            node_possible_entities[channel[i]].add(funding_entity)\n",
    "\n",
    "#     write_json(funding_entity_channels_nodes, funding_entity_channels_nodes_file)\n",
    "\n",
    "    heuristic_2a_entity_node = dict()\n",
    "    # create link between entity and a node when\n",
    "    # the node is the only one present in every channel of the entity\n",
    "    for fe in funding_entity_channels_nodes:\n",
    "        # count number of occurrences of each node in channels\n",
    "        node_occur = dict()\n",
    "\n",
    "        # compute node_occur\n",
    "        for channel in funding_entity_channels_nodes[fe]:\n",
    "            for node in funding_entity_channels_nodes[fe][channel]:\n",
    "                if node not in node_occur:\n",
    "                    node_occur[node] = 0\n",
    "                node_occur[node] += 1\n",
    "\n",
    "        # get max_occur\n",
    "        max_occur = max(node_occur.values())\n",
    "        selected_node = None\n",
    "\n",
    "        # check if there is a perfect max_occur, i.e.,\n",
    "        # if max_occur is unique and in every channel\n",
    "        # (corresponding node is in every channel)\n",
    "        if list(node_occur.values()).count(max_occur) == 1 \\\n",
    "                and max_occur == len(funding_entity_channels_nodes[fe]) \\\n",
    "                and max_occur >= min_conf:\n",
    "            # get node present in every channel and add it to its entity\n",
    "            selected_node = [n for n, occ in node_occur.items()\n",
    "                             if occ == max_occur][0]\n",
    "            if fe not in heuristic_2a_entity_node:\n",
    "                heuristic_2a_entity_node[fe] = set()\n",
    "            heuristic_2a_entity_node[fe] \\\n",
    "                .add(selected_node)\n",
    "\n",
    "    # print('Step 2:')\n",
    "    heuristic_2b_entity_node = link_other_nodes(heuristic_2a_entity_node, channels_df,\n",
    "                         funded_address_settlement_txs, funding_txs,\n",
    "                         settlement_address_entity)\n",
    "\n",
    "    heuristic_2b_node_entity = invert_mapping(heuristic_2b_entity_node)\n",
    "\n",
    "    r = get_results(r, heuristic_2b_entity_node,\n",
    "                    heuristic_2b_node_entity)\n",
    "\n",
    "    addresses_linked = set()\n",
    "    for address_entity in [funding_address_entity, settlement_address_entity]:\n",
    "        for address, entity in address_entity.items():\n",
    "            if entity in heuristic_2b_entity_node:\n",
    "                addresses_linked.add(address)\n",
    "    r['perc_addresses_linked'] = round(\n",
    "        100*len(addresses_linked)/r['n_addresses'], 2)\n",
    "\n",
    "    output_file_a, output_file_b = files[2]['all']\n",
    "    for k in ['stars', 'none', 'snakes', 'collectors', 'proxies', 'all']:\n",
    "        if och[k]:\n",
    "            output_file_a, output_file_b = files[2][k]\n",
    "\n",
    "    # Write to file\n",
    "    heuristic_2_entity_node = \\\n",
    "        {str(k): [e for e in v] for k, v in heuristic_2b_entity_node.items()}\n",
    "    heuristic_2_node_entity = \\\n",
    "        {k: [int(e) for e in v] for k, v in heuristic_2b_node_entity.items()}\n",
    "    print('On-chain clustering', och)\n",
    "    print('writing to', output_file_a, output_file_b)\n",
    "    write_json(heuristic_2_entity_node, output_file_a)\n",
    "    write_json(heuristic_2_node_entity, output_file_b)\n",
    "\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:18:22.929874Z",
     "start_time": "2020-10-01T15:17:58.049100Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 1 -- Number of linked entities: 841\n",
      "Iteration: 2 -- Number of linked entities: 4825\n",
      "Iteration: 3 -- Number of linked entities: 8629\n",
      "Iteration: 4 -- Number of linked entities: 9636\n",
      "Iteration: 5 -- Number of linked entities: 9855\n",
      "Iteration: 6 -- Number of linked entities: 9900\n",
      "Iteration: 7 -- Number of linked entities: 9904\n",
      "On-chain clustering {'none': True, 'stars': False, 'snakes': False, 'collectors': False, 'proxies': False, 'all': False}\n",
      "writing to ../data/results/none_2_entity_node.json ../data/results/none_2_node_entity.json\n",
      "\n",
      "use stars\n",
      "Iteration: 1 -- Number of linked entities: 862\n",
      "Iteration: 2 -- Number of linked entities: 4846\n",
      "Iteration: 3 -- Number of linked entities: 8650\n",
      "Iteration: 4 -- Number of linked entities: 9657\n",
      "Iteration: 5 -- Number of linked entities: 9876\n",
      "Iteration: 6 -- Number of linked entities: 9921\n",
      "Iteration: 7 -- Number of linked entities: 9925\n",
      "On-chain clustering {'none': False, 'stars': True, 'snakes': False, 'collectors': False, 'proxies': False, 'all': False}\n",
      "writing to ../data/results/stars_2_entity_node.json ../data/results/stars_2_node_entity.json\n",
      "\n",
      "use snakes\n",
      "Iteration: 1 -- Number of linked entities: 6285\n",
      "Iteration: 2 -- Number of linked entities: 10322\n",
      "Iteration: 3 -- Number of linked entities: 14101\n",
      "Iteration: 4 -- Number of linked entities: 15096\n",
      "Iteration: 5 -- Number of linked entities: 15309\n",
      "Iteration: 6 -- Number of linked entities: 15349\n",
      "Iteration: 7 -- Number of linked entities: 15352\n",
      "On-chain clustering {'none': False, 'stars': False, 'snakes': True, 'collectors': False, 'proxies': False, 'all': False}\n",
      "writing to ../data/results/snakes_2_entity_node.json ../data/results/snakes_2_node_entity.json\n",
      "\n",
      "use collectors\n",
      "Iteration: 1 -- Number of linked entities: 841\n",
      "Iteration: 2 -- Number of linked entities: 4817\n",
      "Iteration: 3 -- Number of linked entities: 8676\n",
      "Iteration: 4 -- Number of linked entities: 9755\n",
      "Iteration: 5 -- Number of linked entities: 10009\n",
      "Iteration: 6 -- Number of linked entities: 10053\n",
      "Iteration: 7 -- Number of linked entities: 10055\n",
      "On-chain clustering {'none': False, 'stars': False, 'snakes': False, 'collectors': True, 'proxies': False, 'all': False}\n",
      "writing to ../data/results/collectors_2_entity_node.json ../data/results/collectors_2_node_entity.json\n",
      "\n",
      "use proxies\n",
      "Iteration: 1 -- Number of linked entities: 841\n",
      "Iteration: 2 -- Number of linked entities: 5203\n",
      "Iteration: 3 -- Number of linked entities: 9731\n",
      "Iteration: 4 -- Number of linked entities: 11045\n",
      "Iteration: 5 -- Number of linked entities: 11399\n",
      "Iteration: 6 -- Number of linked entities: 11447\n",
      "Iteration: 7 -- Number of linked entities: 11451\n",
      "On-chain clustering {'none': False, 'stars': False, 'snakes': False, 'collectors': False, 'proxies': True, 'all': False}\n",
      "writing to ../data/results/proxies_2_entity_node.json ../data/results/proxies_2_node_entity.json\n",
      "\n",
      "use stars\n",
      "use snakes\n",
      "use collectors\n",
      "use proxies\n",
      "Iteration: 1 -- Number of linked entities: 6306\n",
      "Iteration: 2 -- Number of linked entities: 10773\n",
      "Iteration: 3 -- Number of linked entities: 15584\n",
      "Iteration: 4 -- Number of linked entities: 17156\n",
      "Iteration: 5 -- Number of linked entities: 17532\n",
      "Iteration: 6 -- Number of linked entities: 17602\n",
      "Iteration: 7 -- Number of linked entities: 17606\n",
      "On-chain clustering {'none': False, 'stars': True, 'snakes': True, 'collectors': True, 'proxies': True, 'all': True}\n",
      "writing to ../data/results/all_2_entity_node.json ../data/results/all_2_node_entity.json\n"
     ]
    }
   ],
   "source": [
    "on_chain_heuristics = {och: False for och in on_chain_heuristics_list}\n",
    "\n",
    "results_2 = dict()\n",
    "for och in on_chain_heuristics:\n",
    "    # one by one\n",
    "    if och != 'all':\n",
    "        on_chain_heuristics[och] = True\n",
    "        results_2[och] = heuristic_2(funding_address_entity, settlement_address_entity, on_chain_heuristics, heuristics_files)\n",
    "        on_chain_heuristics[och] = False\n",
    "\n",
    "# all\n",
    "on_chain_heuristics = {och: (True if och != 'none' else False) for och in on_chain_heuristics_list}\n",
    "results_2['all'] = heuristic_2(funding_address_entity, settlement_address_entity, on_chain_heuristics, heuristics_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json(results, heuristics_files[2]['results'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T20:18:45.035685Z",
     "start_time": "2020-09-28T20:18:45.032655Z"
    }
   },
   "source": [
    "# 3. Validation\n",
    "In this section we prepare the ground truth (GT) data we collected to validate our linking heuristics and then we compare it with our results. We collected data by opening channels (see section \"Outgoing Channels\") and by letting other people opening channels to us (see section \"Incoming Channels\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Prepare Ground Truth Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:26:35.763736Z",
     "start_time": "2020-10-01T15:26:35.575630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use stars\n",
      "use snakes\n",
      "use collectors\n",
      "use proxies\n"
     ]
    }
   ],
   "source": [
    "# use all on-chain clustering heuristics\n",
    "on_chain_heuristics = {och: (True if och != 'none' else False) for och in on_chain_heuristics_list}\n",
    "funding_address_entity, settlement_address_entity = set_mapping(funding_address_entity, settlement_address_entity, on_chain_heuristics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outgoing Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:26:37.043817Z",
     "start_time": "2020-10-01T15:26:37.040329Z"
    }
   },
   "outputs": [],
   "source": [
    "gt_outgoing_channels = read_json(outgoing_channels_file)['channels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:26:37.465003Z",
     "start_time": "2020-10-01T15:26:37.457797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First block with ground truth data: 646559\n",
      "Last block with ground truth data: 647433\n",
      "GT channels opened and closed: 81\n",
      "GT number of nodes with which we closed a channel: 73\n"
     ]
    }
   ],
   "source": [
    "local_node = '025228840b37ade9aa2f96b3c961a35e76571a7c87a4ee67e2f33c64de64aa822f'\n",
    "first_block = 999999\n",
    "last_block = 0\n",
    "for el in gt_outgoing_channels:\n",
    "    hsh, _ = el['channel_point'].split(':')\n",
    "    funding_block = funding_txs[hsh]['status']['block_height']\n",
    "    settlement_block = el['close_height']\n",
    "    if funding_block < first_block:\n",
    "        first_block = funding_block\n",
    "    if settlement_block > last_block:\n",
    "        last_block = settlement_block\n",
    "\n",
    "gt_outgoing_channel_points = [el['channel_point'] for el in gt_outgoing_channels]\n",
    "\n",
    "closed_channel_nodes = set()\n",
    "for closed_channel in gt_outgoing_channels:\n",
    "    closed_channel_nodes.add(closed_channel['remote_pubkey'])\n",
    "closed_channel_nodes.add(local_node)\n",
    "\n",
    "print('First block with ground truth data:', first_block)\n",
    "print('Last block with ground truth data:', last_block)\n",
    "print('GT channels opened and closed:', len(gt_outgoing_channel_points))\n",
    "print('GT number of nodes with which we closed a channel:', len(closed_channel_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:26:38.366659Z",
     "start_time": "2020-10-01T15:26:38.162535Z"
    }
   },
   "outputs": [],
   "source": [
    "chpoint_n1_n2 = dict()\n",
    "for r in channels_df.values:\n",
    "    channel_point, node1, node2 = r\n",
    "    if channel_point in gt_outgoing_channel_points:\n",
    "        chpoint_n1_n2[channel_point] = [node1, node2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:26:38.636153Z",
     "start_time": "2020-10-01T15:26:38.624548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT number of nodes that received coins from us: 52\n"
     ]
    }
   ],
   "source": [
    "gt_entity_node = dict()\n",
    "for cp, ns in chpoint_n1_n2.items():\n",
    "    hsh, out_index = cp.split(':')\n",
    "    funding_address = funding_txs[hsh]['vin'][0]['prevout']['scriptpubkey_address']\n",
    "    funding_entity = funding_address_entity[funding_address]\n",
    "    gt_entity_node = add_node_to_entity(local_node, funding_entity, gt_entity_node)\n",
    "\n",
    "received_coins_nodes = set()\n",
    "for closed_channel in gt_outgoing_channels:\n",
    "    funding_hsh, out_index = closed_channel['channel_point'].split(':')\n",
    "    funded_address = funding_txs[funding_hsh]['vout'][int(out_index)]['scriptpubkey_address']\n",
    "    stx = funded_address_settlement_txs[funded_address][0]\n",
    "    if not stx['txid'] == closed_channel['closing_tx_hash']:\n",
    "        print(stx['txid'])\n",
    "    else:\n",
    "        # if there are two outputs, the first is remote and the second is local\n",
    "        if len(stx['vout']) == 2:\n",
    "            received_coins_nodes.add(closed_channel['remote_pubkey'])\n",
    "            remote_settlement_entity = settlement_address_entity[stx['vout'][0]['scriptpubkey_address']]\n",
    "            local_settlement_entity = settlement_address_entity[stx['vout'][1]['scriptpubkey_address']]\n",
    "            gt_entity_node = add_node_to_entity(local_node, local_settlement_entity, gt_entity_node)\n",
    "            gt_entity_node = add_node_to_entity(closed_channel['remote_pubkey'], remote_settlement_entity, gt_entity_node)\n",
    "        # if there is one output, it is local\n",
    "        elif len(stx['vout']) == 1:\n",
    "            local_settlement_entity = settlement_address_entity[stx['vout'][0]['scriptpubkey_address']]\n",
    "            gt_entity_node = add_node_to_entity(local_node, local_settlement_entity, gt_entity_node)        \n",
    "\n",
    "print('GT number of nodes that received coins from us:', len(received_coins_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T13:34:59.263513Z",
     "start_time": "2020-09-29T13:34:59.261025Z"
    }
   },
   "source": [
    "### Incoming channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:26:41.897196Z",
     "start_time": "2020-10-01T15:26:41.891434Z"
    }
   },
   "outputs": [],
   "source": [
    "incoming_channels_df = pd.read_csv(incoming_channels_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:26:44.492630Z",
     "start_time": "2020-10-01T15:26:44.476750Z"
    }
   },
   "outputs": [],
   "source": [
    "available_funding_txs = set([el.split(':')[0] for el in incoming_channels_df.chan_point.values]).intersection(set(funding_txs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T16:03:51.193939Z",
     "start_time": "2020-10-01T16:03:51.188684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes that opened channels to us: 3\n"
     ]
    }
   ],
   "source": [
    "# external_node_key is linked to the funding entity\n",
    "nodes_opened_channels_to_us = set()\n",
    "for r in incoming_channels_df.values:\n",
    "    chan_point, remote_node, remote_alias = r\n",
    "    hsh, out_index = chan_point.split(':')\n",
    "    if hsh in funding_txs:\n",
    "        nodes_opened_channels_to_us.add(remote_node)\n",
    "        funding_entity = funding_address_entity[funding_txs[hsh]['vin'][0]['prevout']['scriptpubkey_address']]\n",
    "        gt_entity_node = add_node_to_entity(remote_node, funding_entity, gt_entity_node)\n",
    "\n",
    "print('Number of nodes that opened channels to us:', len(nodes_opened_channels_to_us))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node-Entity links\n",
    "Here we create the gt linking between node and entity using the results of the cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:26:51.163619Z",
     "start_time": "2020-10-01T15:26:51.160824Z"
    }
   },
   "outputs": [],
   "source": [
    "gt_node_entity = dict()\n",
    "for e, ns in gt_entity_node.items():\n",
    "    for n in ns:\n",
    "        gt_node_entity = add_node_to_entity(e, n, gt_node_entity) # don't be fooled by the name ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:18:58.133367Z",
     "start_time": "2020-10-01T15:18:58.129139Z"
    }
   },
   "outputs": [],
   "source": [
    "# write_json(gt_node_entity, gt_node_entity_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:18:58.687645Z",
     "start_time": "2020-10-01T15:18:58.683705Z"
    }
   },
   "outputs": [],
   "source": [
    "# gt_node_entity = read_json(gt_node_entity_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Compare with Ground Truth\n",
    "For each node in the ground truth, compare its `gt_entities` and its `linked_entities`. If there is at least one entity in common in the two sets, the node-entity link is valid. We then extend this also by looking at neighboring entities (entities that directly receive or send coins to a specific entity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:26:53.094518Z",
     "start_time": "2020-10-01T15:26:53.079490Z"
    }
   },
   "outputs": [],
   "source": [
    "heuristic_2_node_entity = read_json(heuristics_files[2]['all'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T16:06:06.850421Z",
     "start_time": "2020-10-01T16:06:06.844711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of directly validated nodes: 7\n"
     ]
    }
   ],
   "source": [
    "validated_nodes = set()\n",
    "for n in gt_node_entity:\n",
    "    if n in heuristic_2_node_entity:\n",
    "        gt_entities = list(set(gt_node_entity[n]))\n",
    "        linked_entities = list(set(heuristic_2_node_entity[n]))\n",
    "        gt_entities.sort()\n",
    "        linked_entities.sort()\n",
    "        len_intersection = len(set(linked_entities).intersection(set(gt_entities)))\n",
    "        if len_intersection:\n",
    "            validated_nodes.add(n)\n",
    "\n",
    "print('Number of directly validated nodes:', len(validated_nodes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find indirect connection between gt entity and linked entity for unvalidated nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:29:53.228484Z",
     "start_time": "2020-10-01T15:29:53.224346Z"
    }
   },
   "outputs": [],
   "source": [
    "entity_nbrs = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T11:36:28.760399Z",
     "start_time": "2020-10-05T11:36:28.756953Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entity_nbrs = read_json(entity_nbrs_file) # if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T16:06:08.592285Z",
     "start_time": "2020-10-01T16:06:08.575586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701710153 1 03fce165537aea120bffe8505876b44d5119354f825b3eac329b761fc5636bf334\n",
      "702110475 1 0311cad0edf4ac67298805cf4407d94358ca60cd44f2e360856f3b1c088bcd4782\n"
     ]
    }
   ],
   "source": [
    "# **WARNING** GraphSense token is needed to run this cell, unless you have entity_nbrs_file\n",
    "gt_entity_hop_nbrs = dict() # key: gt_entity, value: dict of key: hop, value: neighbors\n",
    "h = 1\n",
    "for n, es in gt_node_entity.items():\n",
    "    if n not in validated_nodes:\n",
    "        for e in es:\n",
    "            if e not in gt_entity_hop_nbrs:\n",
    "                gt_entity_hop_nbrs[e] = dict()\n",
    "                gt_entity_hop_nbrs[e][h] = set()\n",
    "            # get neighbors at hop 1\n",
    "            if not gt_entity_hop_nbrs[e][h] and e > 0:\n",
    "                if e not in entity_nbrs:\n",
    "                    entity_nbrs[e] = get_entity_neighbors(e)\n",
    "                gt_entity_hop_nbrs[e][h] = gt_entity_hop_nbrs[e][h].union(entity_nbrs[e])\n",
    "\n",
    "indirectly_validated_nodes = set()\n",
    "for n, es in gt_node_entity.items():\n",
    "    if n not in validated_nodes:\n",
    "        for e in es:\n",
    "            nbrs = gt_entity_hop_nbrs[e][h]\n",
    "            nbrs_linked_entities_intersection = nbrs.intersection(heuristic_2_node_entity[n])\n",
    "            if nbrs_linked_entities_intersection:\n",
    "                print(e, len(nbrs_linked_entities_intersection), n)\n",
    "                indirectly_validated_nodes.add(n)\n",
    "                validated_nodes.add(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T16:06:29.219907Z",
     "start_time": "2020-10-01T16:06:29.216856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of validated nodes: 9\n"
     ]
    }
   ],
   "source": [
    "print('Total number of validated nodes:', len(validated_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T11:36:28.760399Z",
     "start_time": "2020-10-05T11:36:28.756953Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "write_json(entity_nbrs, entity_nbrs_file, values_to_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:32:30.197791Z",
     "start_time": "2020-10-01T15:32:30.191942Z"
    }
   },
   "outputs": [],
   "source": [
    "# manually checked, spent before last_block = 647529\n",
    "settlement_txs_where_remote_spends_coins =[\n",
    "    'd6d658c4a13c8f2d2927a71e1cdd5ef310d7d9adb9f96774018276a3590c3788',\n",
    "    'be051cfb727c10c28c3975d8d32a0c29bb244b4f21bd14e2eab584219f496b27',\n",
    "    '9fa69e68dc5ce6525b3edeb7dba8f2d954adea3d82ca340a55b7157418d384c1',\n",
    "    '101c492db10266eb1c7cd63e00bcfbb9f60860a0badff6c7573b673f523b45f2',\n",
    "    'f65aef03e5a93d5acebf8135cd411ccf46013a957620eab2a5a95171327f4e93',\n",
    "    'e836d71d6cc8b5a79562b46890429a89ecc5e9e3be8cbc0203a1c00bd69c8d2a',\n",
    "    'fe9f60f930d1a7cad6b17923c8f3f041b5e2ba308447c73d82abc048389c930a',\n",
    "    '75f27715d27c6629673ddf080cf5267dc85bea40bf702fcb59377f82553b7e08',\n",
    "    '2894fac92b98402a993b6b57db0877db085d796978c89495b630871e12b2427f',\n",
    "    '4dd694546be280a08803b9e2eb9e15adfe0f4e4ef0f53567d9a7f183188ebcff',\n",
    "    '7e25d41fd47d10287e560c3d98cebe041c0f7ae57c1fff270ad662753ec706c0',\n",
    "    'f4c30c226bb4ce2c16673555768190318dd327bab9d06b51ff35a497483eff70',\n",
    "    '399cfcd171e69d7a0c150772bba5202850c0186e58d9b818cfaf6a4c74f567fa',\n",
    "    '05489ca075ff037934e734b893098559c10eabc953edca1cc2faa80fe1042582',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:32:30.939885Z",
     "start_time": "2020-10-01T15:32:30.935513Z"
    }
   },
   "outputs": [],
   "source": [
    "node_settlement_address_txs = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:37:49.314464Z",
     "start_time": "2020-10-01T15:37:49.312139Z"
    }
   },
   "outputs": [],
   "source": [
    "gt_address_txs = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:37:49.314464Z",
     "start_time": "2020-10-01T15:37:49.312139Z"
    }
   },
   "outputs": [],
   "source": [
    "gt_address_txs = read_json(gt_address_txs_file) # if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:38:13.435168Z",
     "start_time": "2020-10-01T15:38:13.426842Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT number of nodes that received our coins and did not spend them: 41\n",
      "GT number of addresses that received our coins and did not spend them 43\n"
     ]
    }
   ],
   "source": [
    "# **WARNING** GraphSense token is needed to run this cell, unless you have gt_address_txs_file\n",
    "# check activity of node addresses not spending our coins\n",
    "nodes_that_spent_our_coins = set()\n",
    "for closed_channel in gt_outgoing_channels:\n",
    "    funding_hsh, out_index = closed_channel['channel_point'].split(':')\n",
    "    funded_address = funding_txs[funding_hsh]['vout'][int(out_index)]['scriptpubkey_address']\n",
    "    stx = funded_address_settlement_txs[funded_address][0]\n",
    "    # if there are two outputs, the first is remote and the second is local\n",
    "    if len(stx['vout']) == 2:\n",
    "        if stx['txid'] not in settlement_txs_where_remote_spends_coins:\n",
    "            if closed_channel['remote_pubkey'] not in node_settlement_address_txs:\n",
    "                node_settlement_address_txs[closed_channel['remote_pubkey']] = dict()\n",
    "            a = stx['vout'][0]['scriptpubkey_address']\n",
    "            if a not in node_settlement_address_txs[closed_channel['remote_pubkey']]:\n",
    "                if a not in gt_address_txs:\n",
    "                    gt_address_txs[a] = get_address_txs(a)\n",
    "                node_settlement_address_txs[closed_channel['remote_pubkey']][a] = gt_address_txs[a]\n",
    "            elif not node_settlement_address_txs[closed_channel['remote_pubkey']][a]:\n",
    "                if a not in address_txs:\n",
    "                    gt_address_txs[a] = get_address_txs(a)\n",
    "                node_settlement_address_txs[closed_channel['remote_pubkey']][a] = gt_address_txs[a]\n",
    "        else:\n",
    "            nodes_that_spent_our_coins.add(closed_channel['remote_pubkey'])\n",
    "for node in nodes_that_spent_our_coins:\n",
    "    node_settlement_address_txs.pop(node, None)\n",
    "\n",
    "print('Number of nodes that received our coins and did not spend them:', len(node_settlement_address_txs.keys()))\n",
    "print('Number of addresses that received our coins and did not spend them', sum([len(d.keys()) for d in node_settlement_address_txs.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T11:35:10.284225Z",
     "start_time": "2020-10-05T11:35:10.278232Z"
    }
   },
   "outputs": [],
   "source": [
    "write_json(gt_address_txs, gt_address_txs_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:41:26.709253Z",
     "start_time": "2020-10-01T15:41:26.704359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT number of nodes that did not reuse addresses, received our coins and never spent any coins: 41\n"
     ]
    }
   ],
   "source": [
    "node_address_no_spend = set()\n",
    "# the node receives coins on addresses that have only 1 incoming tx\n",
    "for node, d in node_settlement_address_txs.items():\n",
    "    discard = False # discard if address is not new or spent coins\n",
    "    for a, txs in d.items():\n",
    "        # discard if the address has more than 1 incoming tx or spent \n",
    "        if txs['no_incoming_txs'] > 1 or txs['no_outgoing_txs']: \n",
    "            discard = True\n",
    "    if not discard:\n",
    "        node_address_no_spend.add(node)\n",
    "\n",
    "print('Number of nodes that did not reuse addresses, received our coins and never spent any coins:', len(node_address_no_spend))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:41:27.655757Z",
     "start_time": "2020-10-01T15:41:27.650391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT number of nodes that received and spent our coins: 11\n",
      "GT number of nodes that received and spent our coins and are validated: 7\n"
     ]
    }
   ],
   "source": [
    "received_spent_coins_nodes = set()\n",
    "for closed_channel in gt_outgoing_channels:\n",
    "    funding_hsh, out_index = closed_channel['channel_point'].split(':')\n",
    "    funded_address = funding_txs[funding_hsh]['vout'][int(out_index)]['scriptpubkey_address']\n",
    "    stx = funded_address_settlement_txs[funded_address][0]\n",
    "    # if there are two outputs, the first is remote and the second is local\n",
    "    if len(stx['vout']) == 2 and stx['txid'] in settlement_txs_where_remote_spends_coins:\n",
    "        received_spent_coins_nodes.add(closed_channel['remote_pubkey'])\n",
    "\n",
    "print('Number of nodes that received and spent our coins:', len(received_spent_coins_nodes))\n",
    "print('Number of nodes that received and spent our coins and are validated:', len(received_spent_coins_nodes.intersection(validated_nodes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:41:33.747948Z",
     "start_time": "2020-10-01T15:41:33.741415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT number of nodes that received our coins and were linked with our heuristic: 52\n"
     ]
    }
   ],
   "source": [
    "received_coins_linked_nodes = received_coins_nodes.intersection(set(heuristic_2_node_entity.keys()))\n",
    "print('Number of nodes that received our coins and were linked with our heuristic:', len(received_coins_linked_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List info about non-validated nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:44:21.799228Z",
     "start_time": "2020-10-01T15:44:21.249849Z"
    }
   },
   "outputs": [],
   "source": [
    "nodes = pd.read_csv(nodes_csv_file)\n",
    "node_alias, alias_node = df_to_dicts_set(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:44:24.689044Z",
     "start_time": "2020-10-01T15:44:24.682007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "032d4baebebfdeab7a2ecef2fbe109cbef10de95f05aa54090fdb687789547dbf5\n",
      "{'CONNECT_WITH_ME'}\n",
      "{702410217}\n",
      "4600\n",
      "\n",
      "0303a518845db99994783f606e6629e705cfaf072e5ce9a4d8bf9e249de4fbd019\n",
      "{'LNBIG.com [lnd-25]'}\n",
      "{702145255}\n",
      "4600\n",
      "\n",
      "031ce29116eab7edd66148f5169f1fb658fad62bdc5091221ab895fe5d36db00b2\n",
      "{'LNBIG.com [lnd-05]'}\n",
      "{701940461}\n",
      "4600\n",
      "\n",
      "03864ef025fde8fb587d989186ce6a4a186895ee44a926bfc370e2c366597a3f8f\n",
      "{'ACINQ'}\n",
      "{700855070}\n",
      "4600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in received_spent_coins_nodes:\n",
    "    if n not in validated_nodes:\n",
    "        print(n)\n",
    "        print(node_alias[n])\n",
    "        print(gt_node_entity[n])\n",
    "        print(len(heuristic_2_node_entity))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Compare with each Other\n",
    "Here we compare the linking results of heuristic 1 with the ones of heuristic 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:58:03.731368Z",
     "start_time": "2020-10-01T15:58:03.475880Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On-chain heuristic used: none\n",
      "9042 entities out of 9042 entities in common are linked to the same nodes\n",
      "9042 entities out of 9042 entities in common are linked to at least on same node\n",
      "On-chain heuristic used: stars\n",
      "9042 entities out of 9042 entities in common are linked to the same nodes\n",
      "9042 entities out of 9042 entities in common are linked to at least on same node\n",
      "On-chain heuristic used: snakes\n",
      "9042 entities out of 9042 entities in common are linked to the same nodes\n",
      "9042 entities out of 9042 entities in common are linked to at least on same node\n",
      "On-chain heuristic used: collectors\n",
      "9193 entities out of 9193 entities in common are linked to the same nodes\n",
      "9193 entities out of 9193 entities in common are linked to at least on same node\n",
      "On-chain heuristic used: proxies\n",
      "10584 entities out of 10584 entities in common are linked to the same nodes\n",
      "10584 entities out of 10584 entities in common are linked to at least on same node\n",
      "On-chain heuristic used: all\n",
      "11272 entities out of 11272 entities in common are linked to the same nodes\n",
      "11272 entities out of 11272 entities in common are linked to at least on same node\n"
     ]
    }
   ],
   "source": [
    "heuristic_1_entity_node_dict = dict()\n",
    "heuristic_2_entity_node_dict = dict()\n",
    "for h in on_chain_heuristics_list:\n",
    "    heuristic_1_entity_node_dict[h] = read_json(heuristics_files[1][h][0] , True)\n",
    "\n",
    "for h in on_chain_heuristics_list:\n",
    "    heuristic_2_entity_node_dict[h] = read_json(heuristics_files[2][h][0], True)\n",
    "\n",
    "for h in on_chain_heuristics_list:\n",
    "    print('On-chain heuristic used:', h)\n",
    "    entities_heuristics_1_2 = set(heuristic_1_entity_node_dict[h]).intersection(set(heuristic_2_entity_node_dict[h]))\n",
    "    # to see if the two heuristics say the same\n",
    "    same = 0\n",
    "    intersect = 0\n",
    "    for e in entities_heuristics_1_2:\n",
    "        s1 = set(heuristic_1_entity_node_dict[h][e])\n",
    "        s2 = set(heuristic_2_entity_node_dict[h][e])\n",
    "        if s1 == s2:\n",
    "            same += 1\n",
    "            intersect += 1\n",
    "        elif s1.intersection(s2):\n",
    "            intersect += 1\n",
    "    print(same, 'entities out of', len(entities_heuristics_1_2),\n",
    "          'entities in common are linked to the same nodes')\n",
    "    print(intersect, 'entities out of', len(entities_heuristics_1_2),\n",
    "          'entities in common are linked to at least on same node')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
