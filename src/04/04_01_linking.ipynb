{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linking Heuristics\n",
    "In this notebook, we present two linking heuristics and we compare the results with our ground truth data. The sections are:\n",
    "1. Linking Heuristic 1\n",
    "    - 1.1 Find Reused Coins\n",
    "    - 1.2 Linking\n",
    "2. Linking Heuristic 2\n",
    "    - Linking\n",
    "3. Validation\n",
    "    - 3.1 Prepare Ground Truth\n",
    "    - 3.2 Compare with Ground Truth\n",
    "    - 3.3 Compare with each other\n",
    "4. Combining Linking and Alias-based heuristics\n",
    "    - 4.1 Augment Linking\n",
    "    - 4.2 Augment Deanonymization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T15:27:20.112959Z",
     "start_time": "2021-01-13T15:27:19.555650Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from api_calls import get_address_txs\n",
    "from utils import read_json, write_json, on_chain_heuristics_list, set_mapping, get_results, most_common, \\\n",
    "    link_other_nodes, invert_mapping, add_node_to_entity, get_entity_neighbors, df_to_dicts_set, patterns_list, \\\n",
    "    df_to_two_dicts\n",
    "\n",
    "# input files\n",
    "from utils import funded_address_settlement_txs_file, funding_address_entity_file, settlement_address_entity_file, \\\n",
    "    channels_file, funding_txs_file, settlement_addresses_file, settlement_txs_file, outgoing_channels_file, \\\n",
    "    incoming_channels_file, address_categories_csv_file, ips_csv_file, alias_address_clusters_csv_file, \\\n",
    "    patterns_files\n",
    "\n",
    "# output files\n",
    "from utils import heuristics_files, gt_node_entity_file, gt_address_txs_file, entity_nbrs_file, nodes_csv_file, \\\n",
    "    funding_node_channel_csv_file\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linking Heuristic 1\n",
    "In this section, we first read and prepare some data and then we perform the linking heuristic 1.\n",
    "\n",
    "#### Inputs (made available):\n",
    "- `channels_file`\n",
    "- `funding_address_entity_file`\n",
    "- `settlement_address_entity_file`\n",
    "- `funding_txs_file`\n",
    "- `settlement_addresses_file`\n",
    "- `funded_address_settlement_txs_file`\n",
    "- `settlement_txs_file`\n",
    "\n",
    "#### Outputs (made available):\n",
    "- `heuristics_files`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:09:08.793652Z",
     "start_time": "2021-01-04T17:09:03.595141Z"
    }
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "channels_df = pd.read_csv(channels_file)\n",
    "funding_address_entity = read_json(funding_address_entity_file)\n",
    "settlement_address_entity = read_json(settlement_address_entity_file)\n",
    "funding_txs = read_json(funding_txs_file)\n",
    "settlement_addresses = set(read_json(settlement_addresses_file))\n",
    "funded_address_settlement_txs = read_json(funded_address_settlement_txs_file)\n",
    "settlement_txs = read_json(settlement_txs_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T15:15:22.153813Z",
     "start_time": "2021-01-04T15:15:21.997572Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_node_channels(channels_df):\n",
    "    # given a node, tell me its channels\n",
    "    node_channels = dict()\n",
    "    for channel in channels_df.values:\n",
    "        c, n1, n2 = channel\n",
    "        if n1 not in node_channels:\n",
    "            node_channels[n1] = set()\n",
    "        node_channels[n1].add(c)\n",
    "        if n2 not in node_channels:\n",
    "            node_channels[n2] = set()\n",
    "        node_channels[n2].add(c)\n",
    "    return node_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:53:36.613128Z",
     "start_time": "2020-10-01T15:53:35.743055Z"
    }
   },
   "outputs": [],
   "source": [
    "# given a node, tell me its channels\n",
    "node_channels = get_node_channels(channels_df)\n",
    "\n",
    "# nodes on-chain activity\n",
    "# for each node, create a list of timestamps of\n",
    "# openings, closings and first/last_activity\n",
    "node_openings_closings = dict()\n",
    "for node, chnls in node_channels.items():\n",
    "    node_openings_closings[node] = {'openings': [], 'closings': []}\n",
    "    for chnl in chnls:\n",
    "        tx_hsh, out_index = chnl.split(':')\n",
    "        t_open = funding_txs[tx_hsh]['status']['block_time']\n",
    "        node_openings_closings[node]['openings'].append(t_open)\n",
    "\n",
    "        t_closed = 0\n",
    "        funded_address = funding_txs[tx_hsh]['vout'][int(out_index)]['scriptpubkey_address']\n",
    "        stxs = funded_address_settlement_txs[funded_address]\n",
    "        if stxs:\n",
    "            t_closed = stxs[0]['status']['block_time']\n",
    "        node_openings_closings[node]['closings'].append(t_closed)\n",
    "    node_openings_closings[node]['first_activity'] = min(\n",
    "        node_openings_closings[node]['openings'])\n",
    "    node_openings_closings[node]['last_activity'] = max(\n",
    "        max(node_openings_closings[node]['openings']),\n",
    "        max(node_openings_closings[node]['closings']))\n",
    "    if min(node_openings_closings[node]['closings']) == 0:\n",
    "        # still open -> now\n",
    "        node_openings_closings[node]['last_activity'] = int(time.time())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Find channels reusing coins from other channels\n",
    "Here we look for channels that were funded with settlement coins (outputs of settlement txs of other channels). We also use the on-chain clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:53:44.436221Z",
     "start_time": "2020-10-01T15:53:36.615379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use stars\n",
      "use snakes\n",
      "use collectors\n",
      "use proxies\n"
     ]
    }
   ],
   "source": [
    "# use all on-chain clustering heuristics to have a wider overlap\n",
    "# then the linking heuristics will decide which triplets to use\n",
    "och = {h: (True if h != 'none' else False) for h in on_chain_heuristics_list}\n",
    "fae, sae, = set_mapping(funding_address_entity, settlement_address_entity, och)\n",
    "\n",
    "fes = set(fae.values())\n",
    "ses = set(sae.values())\n",
    "overlap_entities = fes.intersection(ses)\n",
    "\n",
    "chpoints_reusing_coins = set()\n",
    "settlement_entities = sae.values()\n",
    "for chpoint in channels_df.chan_point.values:\n",
    "    hsh, out_index = chpoint.split(':')\n",
    "    ftx = funding_txs[hsh]\n",
    "    for inp in ftx['vin']:\n",
    "        e = fae[inp['prevout']['scriptpubkey_address']]\n",
    "        if e in overlap_entities and e in settlement_entities:\n",
    "            chpoints_reusing_coins.add(chpoint)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Linking\n",
    "Here is the actual linking heuristic that we run using different on-chain patterns separately and then all of them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:53:51.739877Z",
     "start_time": "2020-10-01T15:53:51.710323Z"
    }
   },
   "outputs": [],
   "source": [
    "use_entities = True # multi-input clustering\n",
    "\n",
    "def heuristic_1(fae, sae, och, files):\n",
    "    print()\n",
    "    # create a copy of initial state\n",
    "    funding_address_entity = {k: v for k, v in fae.items()}\n",
    "    settlement_address_entity = {k: v for k, v in sae.items()}\n",
    "\n",
    "    # prepare results\n",
    "    r = dict()\n",
    "    r['n_funding_entities'] = len(set(funding_address_entity.values()))\n",
    "    r['n_settlement_entities'] = len(set(settlement_address_entity.values()))\n",
    "    r['n_entities'] = len(set(settlement_address_entity.values()).union(set(funding_address_entity.values())))\n",
    "    r['n_addresses'] = len(set(settlement_address_entity.keys()).union(set(funding_address_entity.keys())))\n",
    "    r['n_nodes'] = len(node_channels)\n",
    "\n",
    "    # map entities to components\n",
    "    funding_address_entity, settlement_address_entity = \\\n",
    "        set_mapping(funding_address_entity, settlement_address_entity, och)\n",
    "\n",
    "    # # mapping between stx and its ftx\n",
    "    stx_its_chpoint = dict()\n",
    "    for channel in channels_df.values:\n",
    "        funding_tx, out_index = channel[0].split(':')\n",
    "        funded_address = funding_txs[funding_tx]['vout'][int(out_index)]['scriptpubkey_address']\n",
    "        settlement_txs_fa = funded_address_settlement_txs[funded_address]\n",
    "        if len(settlement_txs_fa) == 1:  # it is always zero or one tx\n",
    "            stx = settlement_txs_fa[0]['txid']\n",
    "            if stx not in stx_its_chpoint:\n",
    "                stx_its_chpoint[stx] = channel[0]\n",
    "            else:\n",
    "                print('stx already in dict', stx)\n",
    "\n",
    "    # create links for heuristic 1 (both at address and entity level)\n",
    "    stx_a_chpoint = []  # list of settlement tx, address, funding tx\n",
    "    for chpoint in chpoints_reusing_coins:\n",
    "        hsh, out_index = chpoint.split(':')\n",
    "        uftx = funding_txs[hsh]\n",
    "        for i in uftx['vin']:\n",
    "            a = i['prevout']['scriptpubkey_address']\n",
    "            prev_tx = i['txid']\n",
    "            if a in settlement_addresses:\n",
    "                if prev_tx in settlement_txs:\n",
    "                    stx_a_chpoint.append([prev_tx, a, chpoint])\n",
    "    #             else:\n",
    "    #                 # a is a settlement_address but prev_tx is not a\n",
    "    #                 settlement_tx in our data\n",
    "\n",
    "    stx_e_chpoint = []  # list of settlement tx, entity, chpoint\n",
    "    print('n coins reused', len(chpoints_reusing_coins))\n",
    "    settlement_entities = set(settlement_address_entity.values())\n",
    "    for chpoint in chpoints_reusing_coins:\n",
    "        hsh, out_index = chpoint.split(':')\n",
    "        uftx = funding_txs[hsh]\n",
    "        for i in uftx['vin']:\n",
    "            e = funding_address_entity[i['prevout']['scriptpubkey_address']]\n",
    "            prev_tx = i['txid']\n",
    "            if e in settlement_entities:\n",
    "                if prev_tx in settlement_txs:\n",
    "                    stx_e_chpoint.append([prev_tx, e, chpoint])\n",
    "\n",
    "    # I need a mapping between ch_point and nodes\n",
    "    # and between settlement tx and nodes\n",
    "    chpoint_nodes = dict()\n",
    "    for channel in channels_df.values:\n",
    "        chpoint_nodes[channel[0]] = [channel[1], channel[2]]\n",
    "\n",
    "    funded_address_chpoint = dict()\n",
    "    for chpoint in channels_df.chan_point.values:\n",
    "        hsh, out_index = chpoint.split(':')\n",
    "        funded_address = funding_txs[hsh]['vout'][int(out_index)]['scriptpubkey_address']\n",
    "        if funded_address not in funded_address_chpoint:\n",
    "            funded_address_chpoint[funded_address] = chpoint\n",
    "        else:\n",
    "            print(funded_address, ' has multiple channels')\n",
    "\n",
    "    stx_nodes = dict()\n",
    "    for fa, chpoint in funded_address_chpoint.items():\n",
    "        stxs = funded_address_settlement_txs[fa]\n",
    "        if stxs:\n",
    "            stx = stxs[0]['txid']\n",
    "            stx_nodes[stx] = chpoint_nodes[chpoint]\n",
    "\n",
    "    # decide link level\n",
    "    triplet = stx_a_chpoint\n",
    "    if use_entities:\n",
    "        triplet = stx_e_chpoint\n",
    "\n",
    "    links = []  # like stx_a_chpoint plus 4 nodes of channels\n",
    "    for el in triplet:\n",
    "        # the funding entity controls the node in common between the channel\n",
    "        # opened with ftx and closed with stx\n",
    "        stx, a, chpoint = el\n",
    "        n1, n2 = chpoint_nodes[chpoint]  # happens after the stx\n",
    "        n3, n4 = stx_nodes[stx]\n",
    "        links.append([stx, a, chpoint, n1, n2, n3, n4])\n",
    "\n",
    "    useful_links = []\n",
    "    for link in links:\n",
    "        s = set(link[3:])\n",
    "        if len(s) == 3:\n",
    "            useful_links.append(link)\n",
    "\n",
    "    # if closing of other node in ch1 > opening of other node in ch2\n",
    "    # then we can use the link\n",
    "    usable_links = []\n",
    "    for link in useful_links:\n",
    "        node_in_common = most_common(link[3:])\n",
    "        other_node_ch1 = ''\n",
    "        other_node_ch2 = ''\n",
    "        for node in link[3:][::-1]:\n",
    "            if node != node_in_common:\n",
    "                if not other_node_ch1:\n",
    "                    other_node_ch1 = node\n",
    "                else:\n",
    "                    other_node_ch2 = node\n",
    "        if node_openings_closings[other_node_ch1]['last_activity'] > \\\n",
    "                node_openings_closings[other_node_ch2]['first_activity']:\n",
    "            usable_links.append(link)\n",
    "\n",
    "    reliable_links_addresses = []\n",
    "    for link in usable_links:\n",
    "        link_address = link[1]\n",
    "        stx = link[0]\n",
    "        its_ftx = stx_its_chpoint[stx].split(':')[0]\n",
    "        if link_address in [el['prevout']['scriptpubkey_address'] for el in\n",
    "                            funding_txs[its_ftx]['vin']]:\n",
    "            reliable_links_addresses.append(link)\n",
    "    print('Number of reliable links at address level:',\n",
    "          len(reliable_links_addresses))\n",
    "\n",
    "    reliable_links_entities = []\n",
    "    entities_reusing = set()\n",
    "    for link in usable_links:\n",
    "        if use_entities:\n",
    "            link_entity = link[1]\n",
    "        else:\n",
    "            link_entity = settlement_address_entity[link[1]]\n",
    "        stx = link[0]\n",
    "        its_ftx = stx_its_chpoint[stx].split(':')[0]\n",
    "        if link_entity in [funding_address_entity[el['prevout']['scriptpubkey_address']] for el\n",
    "                           in funding_txs[its_ftx]['vin']]:\n",
    "            entities_reusing.add(link_entity)\n",
    "            reliable_links_entities.append(link)\n",
    "\n",
    "    print('Number of reliable links at entity level:', len(reliable_links_entities))\n",
    "    print('Number of entities reusing funding addresses:', len(entities_reusing))\n",
    "\n",
    "    # step 1: linking nodes to entity using stx and ftx\n",
    "    # print('Step 1:')\n",
    "    heuristic_1a_entity_node = dict()\n",
    "    heuristic_1a_node_entity = dict()\n",
    "    for link in reliable_links_entities:\n",
    "        if use_entities:\n",
    "            e = link[1]\n",
    "        else:\n",
    "            e = settlement_address_entity[link[1]]\n",
    "        n = most_common(link[3:])\n",
    "        if e not in heuristic_1a_entity_node:\n",
    "            heuristic_1a_entity_node[e] = set()\n",
    "        heuristic_1a_entity_node[e].add(n)\n",
    "        if n not in heuristic_1a_node_entity:\n",
    "            heuristic_1a_node_entity[n] = set()\n",
    "        heuristic_1a_node_entity[n].add(e)\n",
    "    # print('Number of entities linked to nodes:', len(heuristic_1a_entity_node))\n",
    "    # print('Number of nodes linked to entities:', len(heuristic_1a_node_entity))\n",
    "\n",
    "    # print('Step 2:')\n",
    "    # link other node and entity in channel\n",
    "    heuristic_1b_entity_node = link_other_nodes(heuristic_1a_entity_node, channels_df,\n",
    "                                                funded_address_settlement_txs,\n",
    "                                                funding_txs,\n",
    "                                                settlement_address_entity)\n",
    "    heuristic_1b_node_entity = invert_mapping(heuristic_1b_entity_node)\n",
    "\n",
    "    # correct means that the settlement tx has exactly two output entities\n",
    "    correct_stxs = []  # correct stxs\n",
    "    correct_settlement_entities = set()  # output entities of correct stxs\n",
    "    correct_nodes = set()\n",
    "    for channel in channels_df.values:\n",
    "        funding_tx, out_index = channel[0].split(':')\n",
    "        node_1 = channel[1]\n",
    "        node_2 = channel[2]\n",
    "        funded_address = \\\n",
    "            funding_txs[funding_tx]['vout'][int(out_index)]['scriptpubkey_address']\n",
    "\n",
    "        settlement_txs_fa = funded_address_settlement_txs[funded_address]\n",
    "        # if channel is closed and number of outputs == 2 and\n",
    "        # one node is mapped to one entity in the outputs\n",
    "        if settlement_txs_fa:  # it is always only one\n",
    "            for settlement_tx in settlement_txs_fa:\n",
    "                # count entities\n",
    "                entities = set([settlement_address_entity[out['scriptpubkey_address']]\n",
    "                                for out in settlement_tx['vout']])\n",
    "                if len(entities) == 2:\n",
    "                    correct_stxs.append(settlement_tx)\n",
    "                    correct_settlement_entities = correct_settlement_entities.union(entities)\n",
    "                    correct_nodes.add(node_1)\n",
    "                    correct_nodes.add(node_2)\n",
    "\n",
    "    perc_entities_linked_settled = round(100 * len(heuristic_1b_entity_node) / r['n_settlement_entities'], 2)\n",
    "    perc_entities_linked_2e = round(100 * len(heuristic_1b_entity_node) / len(correct_settlement_entities), 2)\n",
    "    perc_nodes_linked_2e = round(100 * len(heuristic_1b_node_entity) / len(correct_nodes), 2)\n",
    "\n",
    "    r = get_results(r, heuristic_1b_entity_node, heuristic_1b_node_entity)\n",
    "\n",
    "    print('Number of settlement entities:', r['n_settlement_entities'], '--', perc_entities_linked_settled, '% linked')\n",
    "    print('Number of settlement entities considering settlement txs with 2 output entities:', len(correct_settlement_entities), '--', perc_entities_linked_2e, '% linked')\n",
    "    print('Number of nodes considering settlement txs with 2 output entities:', len(correct_nodes), '--', perc_nodes_linked_2e, '% linked')\n",
    "\n",
    "    addresses_linked = set()\n",
    "    for address_entity in [funding_address_entity, settlement_address_entity]:\n",
    "        for address, entity in address_entity.items():\n",
    "            if entity in heuristic_1b_entity_node:\n",
    "                addresses_linked.add(address)\n",
    "    r['perc_addresses_linked'] = round(\n",
    "        100 * len(addresses_linked) / r['n_addresses'], 2)\n",
    "\n",
    "    output_file_a, output_file_b = files[1]['all']\n",
    "    for k in ['stars', 'none', 'snakes', 'collectors', 'proxies', 'all']:\n",
    "        if och[k]:\n",
    "            output_file_a, output_file_b = files[1][k]\n",
    "\n",
    "    # Write to file\n",
    "    heuristic_1_entity_node = {str(k): [e for e in v]\n",
    "                               for k, v in heuristic_1b_entity_node.items()}\n",
    "    heuristic_1_node_entity = {k: [int(e) for e in v]\n",
    "                               for k, v in heuristic_1b_node_entity.items()}\n",
    "    print('On-chain clustering', och)\n",
    "    print('writing to', output_file_a, output_file_b)\n",
    "    write_json(heuristic_1_entity_node, output_file_a)\n",
    "    write_json(heuristic_1_node_entity, output_file_b)\n",
    "\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:55:04.444573Z",
     "start_time": "2020-10-01T15:53:53.616731Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n coins reused 12149\n",
      "Number of reliable links at address level: 0\n",
      "Number of reliable links at entity level: 83\n",
      "Number of entities reusing funding addresses: 22\n",
      "Iteration: 1 -- Number of linked entities: 22\n",
      "Iteration: 2 -- Number of linked entities: 3351\n",
      "Iteration: 3 -- Number of linked entities: 7403\n",
      "Iteration: 4 -- Number of linked entities: 8645\n",
      "Iteration: 5 -- Number of linked entities: 8945\n",
      "Iteration: 6 -- Number of linked entities: 9027\n",
      "Iteration: 7 -- Number of linked entities: 9042\n",
      "Number of settlement entities: 53370 -- 16.94 % linked\n",
      "Number of settlement entities considering settlement txs with 2 output entities: 32321 -- 27.98 % linked\n",
      "Number of nodes considering settlement txs with 2 output entities: 4626 -- 46.91 % linked\n",
      "On-chain clustering {'none': True, 'stars': False, 'snakes': False, 'collectors': False, 'proxies': False, 'all': False}\n",
      "writing to ../data/results/none_1_entity_node.json ../data/results/none_1_node_entity.json\n",
      "\n",
      "use stars\n",
      "n coins reused 12149\n",
      "Number of reliable links at address level: 0\n",
      "Number of reliable links at entity level: 83\n",
      "Number of entities reusing funding addresses: 22\n",
      "Iteration: 1 -- Number of linked entities: 22\n",
      "Iteration: 2 -- Number of linked entities: 3351\n",
      "Iteration: 3 -- Number of linked entities: 7403\n",
      "Iteration: 4 -- Number of linked entities: 8645\n",
      "Iteration: 5 -- Number of linked entities: 8945\n",
      "Iteration: 6 -- Number of linked entities: 9027\n",
      "Iteration: 7 -- Number of linked entities: 9042\n",
      "Number of settlement entities: 53370 -- 16.94 % linked\n",
      "Number of settlement entities considering settlement txs with 2 output entities: 32315 -- 27.98 % linked\n",
      "Number of nodes considering settlement txs with 2 output entities: 4626 -- 46.91 % linked\n",
      "On-chain clustering {'none': False, 'stars': True, 'snakes': False, 'collectors': False, 'proxies': False, 'all': False}\n",
      "writing to ../data/results/stars_1_entity_node.json ../data/results/stars_1_node_entity.json\n",
      "\n",
      "use snakes\n",
      "n coins reused 12149\n",
      "Number of reliable links at address level: 0\n",
      "Number of reliable links at entity level: 83\n",
      "Number of entities reusing funding addresses: 22\n",
      "Iteration: 1 -- Number of linked entities: 22\n",
      "Iteration: 2 -- Number of linked entities: 3351\n",
      "Iteration: 3 -- Number of linked entities: 7403\n",
      "Iteration: 4 -- Number of linked entities: 8645\n",
      "Iteration: 5 -- Number of linked entities: 8945\n",
      "Iteration: 6 -- Number of linked entities: 9027\n",
      "Iteration: 7 -- Number of linked entities: 9042\n",
      "Number of settlement entities: 53370 -- 16.94 % linked\n",
      "Number of settlement entities considering settlement txs with 2 output entities: 32321 -- 27.98 % linked\n",
      "Number of nodes considering settlement txs with 2 output entities: 4626 -- 46.91 % linked\n",
      "On-chain clustering {'none': False, 'stars': False, 'snakes': True, 'collectors': False, 'proxies': False, 'all': False}\n",
      "writing to ../data/results/snakes_1_entity_node.json ../data/results/snakes_1_node_entity.json\n",
      "\n",
      "use collectors\n",
      "n coins reused 12149\n",
      "Number of reliable links at address level: 0\n",
      "Number of reliable links at entity level: 83\n",
      "Number of entities reusing funding addresses: 22\n",
      "Iteration: 1 -- Number of linked entities: 22\n",
      "Iteration: 2 -- Number of linked entities: 3341\n",
      "Iteration: 3 -- Number of linked entities: 7446\n",
      "Iteration: 4 -- Number of linked entities: 8763\n",
      "Iteration: 5 -- Number of linked entities: 9099\n",
      "Iteration: 6 -- Number of linked entities: 9180\n",
      "Iteration: 7 -- Number of linked entities: 9193\n",
      "Number of settlement entities: 53370 -- 17.23 % linked\n",
      "Number of settlement entities considering settlement txs with 2 output entities: 31084 -- 29.57 % linked\n",
      "Number of nodes considering settlement txs with 2 output entities: 4620 -- 47.16 % linked\n",
      "On-chain clustering {'none': False, 'stars': False, 'snakes': False, 'collectors': True, 'proxies': False, 'all': False}\n",
      "writing to ../data/results/collectors_1_entity_node.json ../data/results/collectors_1_node_entity.json\n",
      "\n",
      "use proxies\n",
      "n coins reused 12149\n",
      "Number of reliable links at address level: 0\n",
      "Number of reliable links at entity level: 83\n",
      "Number of entities reusing funding addresses: 22\n",
      "Iteration: 1 -- Number of linked entities: 22\n",
      "Iteration: 2 -- Number of linked entities: 3689\n",
      "Iteration: 3 -- Number of linked entities: 8497\n",
      "Iteration: 4 -- Number of linked entities: 10037\n",
      "Iteration: 5 -- Number of linked entities: 10434\n",
      "Iteration: 6 -- Number of linked entities: 10569\n",
      "Iteration: 7 -- Number of linked entities: 10584\n",
      "Number of settlement entities: 53370 -- 19.83 % linked\n",
      "Number of settlement entities considering settlement txs with 2 output entities: 31712 -- 33.38 % linked\n",
      "Number of nodes considering settlement txs with 2 output entities: 4628 -- 53.78 % linked\n",
      "On-chain clustering {'none': False, 'stars': False, 'snakes': False, 'collectors': False, 'proxies': True, 'all': False}\n",
      "writing to ../data/results/proxies_1_entity_node.json ../data/results/proxies_1_node_entity.json\n",
      "\n",
      "use stars\n",
      "use snakes\n",
      "use collectors\n",
      "use proxies\n",
      "n coins reused 12149\n",
      "Number of reliable links at address level: 0\n",
      "Number of reliable links at entity level: 83\n",
      "Number of entities reusing funding addresses: 22\n",
      "Iteration: 1 -- Number of linked entities: 22\n",
      "Iteration: 2 -- Number of linked entities: 3726\n",
      "Iteration: 3 -- Number of linked entities: 8804\n",
      "Iteration: 4 -- Number of linked entities: 10627\n",
      "Iteration: 5 -- Number of linked entities: 11141\n",
      "Iteration: 6 -- Number of linked entities: 11257\n",
      "Iteration: 7 -- Number of linked entities: 11272\n",
      "Number of settlement entities: 53370 -- 21.12 % linked\n",
      "Number of settlement entities considering settlement txs with 2 output entities: 30440 -- 37.03 % linked\n",
      "Number of nodes considering settlement txs with 2 output entities: 4621 -- 55.81 % linked\n",
      "On-chain clustering {'none': False, 'stars': True, 'snakes': True, 'collectors': True, 'proxies': True, 'all': True}\n",
      "writing to ../data/results/all_1_entity_node.json ../data/results/all_1_node_entity.json\n"
     ]
    }
   ],
   "source": [
    "on_chain_heuristics = {och: False for och in on_chain_heuristics_list}\n",
    "\n",
    "results_1 = dict()\n",
    "for och in on_chain_heuristics:\n",
    "    # one by one\n",
    "    if och != 'all':\n",
    "        on_chain_heuristics[och] = True\n",
    "        results_1[och] = heuristic_1(funding_address_entity, settlement_address_entity, on_chain_heuristics, heuristics_files)\n",
    "        on_chain_heuristics[och] = False\n",
    "\n",
    "# all\n",
    "on_chain_heuristics = {och: (True if och != 'none' else False) for och in on_chain_heuristics_list}\n",
    "results_1['all'] = heuristic_1(funding_address_entity, settlement_address_entity, on_chain_heuristics, heuristics_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T12:23:27.102888Z",
     "start_time": "2020-10-01T12:23:27.098588Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_funding_entities': 96181,\n",
       " 'n_settlement_entities': 53370,\n",
       " 'n_entities': 138457,\n",
       " 'n_addresses': 238070,\n",
       " 'n_nodes': 10910,\n",
       " 'n_entities_linked': 11272,\n",
       " 'n_nodes_linked': 2579,\n",
       " 'perc_entities_linked': 8.14,\n",
       " 'perc_nodes_linked': 23.64,\n",
       " 'perc_addresses_linked': 20.96}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json(results, heuristics_files[1]['results'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T20:18:45.035685Z",
     "start_time": "2020-09-28T20:18:45.032655Z"
    }
   },
   "source": [
    "## 2. Linking Heuristic 2\n",
    "Here we run the linking heuristic 2 using on-chain clustering separately and then combined together.\n",
    "\n",
    "#### Inputs (made available):\n",
    "- `channels_file`\n",
    "- `funding_address_entity_file`\n",
    "- `settlement_address_entity_file`\n",
    "- `funding_txs_file`\n",
    "- `funded_address_settlement_txs_file`\n",
    "\n",
    "#### Outputs (made available):\n",
    "- `heuristics_files`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T15:17:17.407889Z",
     "start_time": "2021-01-04T15:17:13.594855Z"
    }
   },
   "outputs": [],
   "source": [
    "channels_df = pd.read_csv(channels_file)\n",
    "funding_address_entity = read_json(funding_address_entity_file)\n",
    "settlement_address_entity = read_json(settlement_address_entity_file)\n",
    "node_channels = get_node_channels(channels_df)\n",
    "funding_txs = read_json(funding_txs_file)\n",
    "funded_address_settlement_txs = read_json(funded_address_settlement_txs_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T15:12:31.783474Z",
     "start_time": "2021-01-04T15:12:31.756124Z"
    }
   },
   "outputs": [],
   "source": [
    "def heuristic_2(fae, sae, och, files):\n",
    "    min_conf = 2  # min confidence level for results\n",
    "\n",
    "    funding_address_entity = {k: v for k, v in fae.items()}\n",
    "    settlement_address_entity = {k: v for k, v in sae.items()}\n",
    "    r = dict()\n",
    "    r['n_funding_entities'] = len(set(funding_address_entity.values()))\n",
    "    r['n_settlement_entities'] = len(set(settlement_address_entity.values()))\n",
    "    r['n_entities'] = len(set(settlement_address_entity.values()).union(set(funding_address_entity.values())))\n",
    "    r['n_addresses'] = len(set(settlement_address_entity.keys()).union(set(funding_address_entity.keys())))\n",
    "    r['n_nodes'] = len(node_channels)\n",
    "\n",
    "    funding_address_entity, settlement_address_entity, = \\\n",
    "        set_mapping(funding_address_entity, settlement_address_entity, och)\n",
    "\n",
    "    funding_entity_possible_nodes = dict()\n",
    "    for channel in channels_df.values:\n",
    "        funding_tx, out_index = channel[0].split(':')\n",
    "        funding_address = funding_txs[funding_tx]['vin'][0]['prevout']['scriptpubkey_address']\n",
    "        funding_entity = funding_address_entity[funding_address]\n",
    "        if funding_entity not in funding_entity_possible_nodes:\n",
    "            funding_entity_possible_nodes[funding_entity] = []\n",
    "        funding_entity_possible_nodes[funding_entity].append(channel[1])\n",
    "        funding_entity_possible_nodes[funding_entity].append(channel[2])\n",
    "\n",
    "    # each funding entity that has at least n_channels possible nodes\n",
    "    # (confidence level >= n_channels)\n",
    "    n_channels = min_conf\n",
    "    entity_channels_half = []\n",
    "    fe_confidence = []\n",
    "    fe_confidence_dict = dict()\n",
    "    for fe, pns in funding_entity_possible_nodes.items():\n",
    "        if len(pns) >= n_channels * 2:  # *2 cause we have two nodes per channel\n",
    "            pn_occur = Counter(pns)\n",
    "            for pn, occur in pn_occur.items():\n",
    "                if occur * 2 == len(pns):\n",
    "                    fe_confidence.append([fe, occur])\n",
    "                    fe_confidence_dict[fe] = occur\n",
    "                    entity_channels_half.append(occur)\n",
    "    entity_channels_half.sort()\n",
    "\n",
    "    funding_entity_channels_nodes = dict()\n",
    "    node_possible_entities = dict()\n",
    "    # populate funding_entity_channels_nodes\n",
    "    for channel in channels_df.values:\n",
    "        funding_tx, out_index = channel[0].split(':')\n",
    "        funding_address = funding_txs[funding_tx]['vin'][0]['prevout']['scriptpubkey_address']\n",
    "        funding_entity = funding_address_entity[funding_address]\n",
    "        if funding_entity not in funding_entity_channels_nodes:\n",
    "            # use chan_point as key\n",
    "            funding_entity_channels_nodes[funding_entity] = dict()\n",
    "        # add nodes\n",
    "        funding_entity_channels_nodes[funding_entity][channel[0]] = [channel[1],\n",
    "                                                                     channel[2]]\n",
    "        for i in [1, 2]:\n",
    "            if channel[i] not in node_possible_entities:\n",
    "                node_possible_entities[channel[i]] = set()\n",
    "            node_possible_entities[channel[i]].add(funding_entity)\n",
    "\n",
    "    heuristic_2a_entity_node = dict()\n",
    "    # create link between entity and a node when\n",
    "    # the node is the only one present in every channel of the entity\n",
    "    for fe in funding_entity_channels_nodes:\n",
    "        # count number of occurrences of each node in channels\n",
    "        node_occur = dict()\n",
    "\n",
    "        # compute node_occur\n",
    "        for channel in funding_entity_channels_nodes[fe]:\n",
    "            for node in funding_entity_channels_nodes[fe][channel]:\n",
    "                if node not in node_occur:\n",
    "                    node_occur[node] = 0\n",
    "                node_occur[node] += 1\n",
    "\n",
    "        # get max_occur\n",
    "        max_occur = max(node_occur.values())\n",
    "        selected_node = None\n",
    "\n",
    "        # check if there is a perfect max_occur, i.e.,\n",
    "        # if max_occur is unique and in every channel\n",
    "        # (corresponding node is in every channel)\n",
    "        if list(node_occur.values()).count(max_occur) == 1 \\\n",
    "                and max_occur == len(funding_entity_channels_nodes[fe]) \\\n",
    "                and max_occur >= min_conf:\n",
    "            # get node present in every channel and add it to its entity\n",
    "            selected_node = [n for n, occ in node_occur.items()\n",
    "                             if occ == max_occur][0]\n",
    "            if fe not in heuristic_2a_entity_node:\n",
    "                heuristic_2a_entity_node[fe] = set()\n",
    "            heuristic_2a_entity_node[fe] \\\n",
    "                .add(selected_node)\n",
    "\n",
    "    heuristic_2b_entity_node = link_other_nodes(heuristic_2a_entity_node, channels_df,\n",
    "                         funded_address_settlement_txs, funding_txs,\n",
    "                         settlement_address_entity)\n",
    "\n",
    "    heuristic_2b_node_entity = invert_mapping(heuristic_2b_entity_node)\n",
    "\n",
    "    r = get_results(r, heuristic_2b_entity_node,\n",
    "                    heuristic_2b_node_entity)\n",
    "\n",
    "    addresses_linked = set()\n",
    "    for address_entity in [funding_address_entity, settlement_address_entity]:\n",
    "        for address, entity in address_entity.items():\n",
    "            if entity in heuristic_2b_entity_node:\n",
    "                addresses_linked.add(address)\n",
    "    r['perc_addresses_linked'] = round(\n",
    "        100*len(addresses_linked)/r['n_addresses'], 2)\n",
    "\n",
    "    output_file_a, output_file_b = files[2]['all']\n",
    "    for k in ['stars', 'none', 'snakes', 'collectors', 'proxies', 'all']:\n",
    "        if och[k]:\n",
    "            output_file_a, output_file_b = files[2][k]\n",
    "\n",
    "    # Write to file\n",
    "    heuristic_2_entity_node = \\\n",
    "        {str(k): [e for e in v] for k, v in heuristic_2b_entity_node.items()}\n",
    "    heuristic_2_node_entity = \\\n",
    "        {k: [int(e) for e in v] for k, v in heuristic_2b_node_entity.items()}\n",
    "    print('On-chain clustering', och)\n",
    "    print('writing to', output_file_a, output_file_b)\n",
    "    write_json(heuristic_2_entity_node, output_file_a)\n",
    "    write_json(heuristic_2_node_entity, output_file_b)\n",
    "\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T15:17:42.591904Z",
     "start_time": "2021-01-04T15:17:18.434125Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 1 -- Number of linked entities: 841\n",
      "Iteration: 2 -- Number of linked entities: 4825\n",
      "Iteration: 3 -- Number of linked entities: 8629\n",
      "Iteration: 4 -- Number of linked entities: 9636\n",
      "Iteration: 5 -- Number of linked entities: 9855\n",
      "Iteration: 6 -- Number of linked entities: 9900\n",
      "Iteration: 7 -- Number of linked entities: 9904\n",
      "On-chain clustering {'none': True, 'stars': False, 'snakes': False, 'collectors': False, 'proxies': False, 'all': False}\n",
      "writing to ../../data/results/none_2_entity_node.json ../../data/results/none_2_node_entity.json\n",
      "\n",
      "use stars\n",
      "Iteration: 1 -- Number of linked entities: 862\n",
      "Iteration: 2 -- Number of linked entities: 4846\n",
      "Iteration: 3 -- Number of linked entities: 8650\n",
      "Iteration: 4 -- Number of linked entities: 9657\n",
      "Iteration: 5 -- Number of linked entities: 9876\n",
      "Iteration: 6 -- Number of linked entities: 9921\n",
      "Iteration: 7 -- Number of linked entities: 9925\n",
      "On-chain clustering {'none': False, 'stars': True, 'snakes': False, 'collectors': False, 'proxies': False, 'all': False}\n",
      "writing to ../../data/results/stars_2_entity_node.json ../../data/results/stars_2_node_entity.json\n",
      "\n",
      "use snakes\n",
      "Iteration: 1 -- Number of linked entities: 6285\n",
      "Iteration: 2 -- Number of linked entities: 10322\n",
      "Iteration: 3 -- Number of linked entities: 14101\n",
      "Iteration: 4 -- Number of linked entities: 15096\n",
      "Iteration: 5 -- Number of linked entities: 15309\n",
      "Iteration: 6 -- Number of linked entities: 15349\n",
      "Iteration: 7 -- Number of linked entities: 15352\n",
      "On-chain clustering {'none': False, 'stars': False, 'snakes': True, 'collectors': False, 'proxies': False, 'all': False}\n",
      "writing to ../../data/results/snakes_2_entity_node.json ../../data/results/snakes_2_node_entity.json\n",
      "\n",
      "use collectors\n",
      "Iteration: 1 -- Number of linked entities: 841\n",
      "Iteration: 2 -- Number of linked entities: 4817\n",
      "Iteration: 3 -- Number of linked entities: 8676\n",
      "Iteration: 4 -- Number of linked entities: 9755\n",
      "Iteration: 5 -- Number of linked entities: 10009\n",
      "Iteration: 6 -- Number of linked entities: 10053\n",
      "Iteration: 7 -- Number of linked entities: 10055\n",
      "On-chain clustering {'none': False, 'stars': False, 'snakes': False, 'collectors': True, 'proxies': False, 'all': False}\n",
      "writing to ../../data/results/collectors_2_entity_node.json ../../data/results/collectors_2_node_entity.json\n",
      "\n",
      "use proxies\n",
      "Iteration: 1 -- Number of linked entities: 841\n",
      "Iteration: 2 -- Number of linked entities: 5203\n",
      "Iteration: 3 -- Number of linked entities: 9731\n",
      "Iteration: 4 -- Number of linked entities: 11045\n",
      "Iteration: 5 -- Number of linked entities: 11399\n",
      "Iteration: 6 -- Number of linked entities: 11447\n",
      "Iteration: 7 -- Number of linked entities: 11451\n",
      "On-chain clustering {'none': False, 'stars': False, 'snakes': False, 'collectors': False, 'proxies': True, 'all': False}\n",
      "writing to ../../data/results/proxies_2_entity_node.json ../../data/results/proxies_2_node_entity.json\n",
      "\n",
      "use stars\n",
      "use snakes\n",
      "use collectors\n",
      "use proxies\n",
      "Iteration: 1 -- Number of linked entities: 6306\n",
      "Iteration: 2 -- Number of linked entities: 10773\n",
      "Iteration: 3 -- Number of linked entities: 15584\n",
      "Iteration: 4 -- Number of linked entities: 17156\n",
      "Iteration: 5 -- Number of linked entities: 17532\n",
      "Iteration: 6 -- Number of linked entities: 17602\n",
      "Iteration: 7 -- Number of linked entities: 17606\n",
      "On-chain clustering {'none': False, 'stars': True, 'snakes': True, 'collectors': True, 'proxies': True, 'all': True}\n",
      "writing to ../../data/results/all_2_entity_node.json ../../data/results/all_2_node_entity.json\n"
     ]
    }
   ],
   "source": [
    "on_chain_heuristics = {och: False for och in on_chain_heuristics_list}\n",
    "\n",
    "results_2 = dict()\n",
    "for och in on_chain_heuristics:\n",
    "    # one by one\n",
    "    if och != 'all':\n",
    "        on_chain_heuristics[och] = True\n",
    "        results_2[och] = heuristic_2(funding_address_entity, settlement_address_entity, on_chain_heuristics, heuristics_files)\n",
    "        on_chain_heuristics[och] = False\n",
    "\n",
    "# all\n",
    "on_chain_heuristics = {och: (True if och != 'none' else False) for och in on_chain_heuristics_list}\n",
    "results_2['all'] = heuristic_2(funding_address_entity, settlement_address_entity, on_chain_heuristics, heuristics_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json(results, heuristics_files[2]['results'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T20:18:45.035685Z",
     "start_time": "2020-09-28T20:18:45.032655Z"
    }
   },
   "source": [
    "# 3. Validation\n",
    "In this section we prepare the ground truth (GT) data we collected to validate our linking heuristics and then we compare it with our results. We collected data by opening channels (see section \"Outgoing Channels\") and by letting other people opening channels to us (see section \"Incoming Channels\").\n",
    "\n",
    "#### Inputs (made available):\n",
    "- `funding_address_entity_file`\n",
    "- `settlement_address_entity_file`\n",
    "- `outgoing_channels_file`\n",
    "- `funding_txs_file`\n",
    "- `channels_file`\n",
    "- `funded_address_settlement_txs_file`\n",
    "- `incoming_channels_file`\n",
    "- `heuristics_files`\n",
    "- `nodes_csv_file`\n",
    "\n",
    "#### Outputs (made available):\n",
    "- `entity_nbrs_file`\n",
    "- `gt_address_txs_file`\n",
    "\n",
    "You can either run all the cells (some might take several minutes to complete, look for the \\*\\*WARNING** comment in the cells) or run directly the \"output files\" cell and have a look at the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Prepare Ground Truth Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:09:32.490302Z",
     "start_time": "2021-01-04T17:09:28.051621Z"
    }
   },
   "outputs": [],
   "source": [
    "funding_address_entity = read_json(funding_address_entity_file)\n",
    "settlement_address_entity = read_json(settlement_address_entity_file)\n",
    "funding_txs = read_json(funding_txs_file)\n",
    "channels_df = pd.read_csv(channels_file)\n",
    "funded_address_settlement_txs = read_json(funded_address_settlement_txs_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:09:32.657452Z",
     "start_time": "2021-01-04T17:09:32.492110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use stars\n",
      "use snakes\n",
      "use collectors\n",
      "use proxies\n"
     ]
    }
   ],
   "source": [
    "# use all on-chain clustering heuristics\n",
    "on_chain_heuristics = {och: (True if och != 'none' else False) for och in on_chain_heuristics_list}\n",
    "funding_address_entity, settlement_address_entity = set_mapping(funding_address_entity, settlement_address_entity, on_chain_heuristics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outgoing Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:09:32.661758Z",
     "start_time": "2021-01-04T17:09:32.659145Z"
    }
   },
   "outputs": [],
   "source": [
    "gt_outgoing_channels = read_json(outgoing_channels_file)['channels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:09:33.318938Z",
     "start_time": "2021-01-04T17:09:33.305178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First block with ground truth data: 646559\n",
      "Last block with ground truth data: 647433\n",
      "GT channels opened and closed: 81\n",
      "GT number of nodes with which we closed a channel: 73\n"
     ]
    }
   ],
   "source": [
    "local_node = '025228840b37ade9aa2f96b3c961a35e76571a7c87a4ee67e2f33c64de64aa822f'\n",
    "first_block = 999999\n",
    "last_block = 0\n",
    "for el in gt_outgoing_channels:\n",
    "    hsh, _ = el['channel_point'].split(':')\n",
    "    funding_block = funding_txs[hsh]['status']['block_height']\n",
    "    settlement_block = el['close_height']\n",
    "    if funding_block < first_block:\n",
    "        first_block = funding_block\n",
    "    if settlement_block > last_block:\n",
    "        last_block = settlement_block\n",
    "\n",
    "gt_outgoing_channel_points = [el['channel_point'] for el in gt_outgoing_channels]\n",
    "\n",
    "closed_channel_nodes = set()\n",
    "for closed_channel in gt_outgoing_channels:\n",
    "    closed_channel_nodes.add(closed_channel['remote_pubkey'])\n",
    "closed_channel_nodes.add(local_node)\n",
    "\n",
    "print('First block with ground truth data:', first_block)\n",
    "print('Last block with ground truth data:', last_block)\n",
    "print('GT channels opened and closed:', len(gt_outgoing_channel_points))\n",
    "print('GT number of nodes with which we closed a channel:', len(closed_channel_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:09:37.417003Z",
     "start_time": "2021-01-04T17:09:37.208858Z"
    }
   },
   "outputs": [],
   "source": [
    "chpoint_n1_n2 = dict()\n",
    "for r in channels_df.values:\n",
    "    channel_point, node1, node2 = r\n",
    "    if channel_point in gt_outgoing_channel_points:\n",
    "        chpoint_n1_n2[channel_point] = [node1, node2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:09:38.398063Z",
     "start_time": "2021-01-04T17:09:38.389387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT number of nodes that received coins from us: 52\n"
     ]
    }
   ],
   "source": [
    "gt_entity_node = dict()\n",
    "for cp, ns in chpoint_n1_n2.items():\n",
    "    hsh, out_index = cp.split(':')\n",
    "    funding_address = funding_txs[hsh]['vin'][0]['prevout']['scriptpubkey_address']\n",
    "    funding_entity = funding_address_entity[funding_address]\n",
    "    gt_entity_node = add_node_to_entity(local_node, funding_entity, gt_entity_node)\n",
    "\n",
    "received_coins_nodes = set()\n",
    "for closed_channel in gt_outgoing_channels:\n",
    "    funding_hsh, out_index = closed_channel['channel_point'].split(':')\n",
    "    funded_address = funding_txs[funding_hsh]['vout'][int(out_index)]['scriptpubkey_address']\n",
    "    stx = funded_address_settlement_txs[funded_address][0]\n",
    "    if not stx['txid'] == closed_channel['closing_tx_hash']:\n",
    "        print(stx['txid'])\n",
    "    else:\n",
    "        # if there are two outputs, the first is remote and the second is local\n",
    "        if len(stx['vout']) == 2:\n",
    "            received_coins_nodes.add(closed_channel['remote_pubkey'])\n",
    "            remote_settlement_entity = settlement_address_entity[stx['vout'][0]['scriptpubkey_address']]\n",
    "            local_settlement_entity = settlement_address_entity[stx['vout'][1]['scriptpubkey_address']]\n",
    "            gt_entity_node = add_node_to_entity(local_node, local_settlement_entity, gt_entity_node)\n",
    "            gt_entity_node = add_node_to_entity(closed_channel['remote_pubkey'], remote_settlement_entity, gt_entity_node)\n",
    "        # if there is one output, it is local\n",
    "        elif len(stx['vout']) == 1:\n",
    "            local_settlement_entity = settlement_address_entity[stx['vout'][0]['scriptpubkey_address']]\n",
    "            gt_entity_node = add_node_to_entity(local_node, local_settlement_entity, gt_entity_node)        \n",
    "\n",
    "print('GT number of nodes that received coins from us:', len(received_coins_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T13:34:59.263513Z",
     "start_time": "2020-09-29T13:34:59.261025Z"
    }
   },
   "source": [
    "### Incoming channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:09:47.344151Z",
     "start_time": "2021-01-04T17:09:47.337307Z"
    }
   },
   "outputs": [],
   "source": [
    "incoming_channels_df = pd.read_csv(incoming_channels_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:09:48.078879Z",
     "start_time": "2021-01-04T17:09:48.058222Z"
    }
   },
   "outputs": [],
   "source": [
    "available_funding_txs = set([el.split(':')[0] for el in incoming_channels_df.chan_point.values]).intersection(set(funding_txs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:09:51.641902Z",
     "start_time": "2021-01-04T17:09:51.635631Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes that opened channels to us: 3\n"
     ]
    }
   ],
   "source": [
    "# external_node_key is linked to the funding entity\n",
    "nodes_opened_channels_to_us = set()\n",
    "for r in incoming_channels_df.values:\n",
    "    chan_point, remote_node, remote_alias = r\n",
    "    hsh, out_index = chan_point.split(':')\n",
    "    if hsh in funding_txs:\n",
    "        nodes_opened_channels_to_us.add(remote_node)\n",
    "        funding_entity = funding_address_entity[funding_txs[hsh]['vin'][0]['prevout']['scriptpubkey_address']]\n",
    "        gt_entity_node = add_node_to_entity(remote_node, funding_entity, gt_entity_node)\n",
    "\n",
    "print('Number of nodes that opened channels to us:', len(nodes_opened_channels_to_us))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node-Entity links\n",
    "Here we create the gt linking between node and entity using the results of the cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:10:26.055314Z",
     "start_time": "2021-01-04T17:10:26.047804Z"
    }
   },
   "outputs": [],
   "source": [
    "gt_node_entity = dict()\n",
    "for e, ns in gt_entity_node.items():\n",
    "    for n in ns:\n",
    "        gt_node_entity = add_node_to_entity(e, n, gt_node_entity) # don't get fooled by the name ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:10:27.369149Z",
     "start_time": "2021-01-04T17:10:27.348480Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gt_node_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T15:18:58.133367Z",
     "start_time": "2020-10-01T15:18:58.129139Z"
    }
   },
   "outputs": [],
   "source": [
    "# write_json(gt_node_entity, gt_node_entity_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:29:40.855019Z",
     "start_time": "2021-01-04T16:29:40.852579Z"
    }
   },
   "outputs": [],
   "source": [
    "# gt_node_entity = read_json(gt_node_entity_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Compare with Ground Truth\n",
    "For each node in the ground truth, compare its `gt_entities` and its `linked_entities`. If there is at least one entity in common in the two sets, the node-entity link is valid. We then extend this also by looking at neighboring entities (entities that directly receive or send coins to a specific entity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:11:25.142335Z",
     "start_time": "2021-01-04T17:11:25.133748Z"
    }
   },
   "outputs": [],
   "source": [
    "heuristic_2_node_entity = read_json(heuristics_files[2]['all'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:11:27.096950Z",
     "start_time": "2021-01-04T17:11:27.089577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of directly validated nodes: 7\n"
     ]
    }
   ],
   "source": [
    "validated_nodes = set()\n",
    "for n in gt_node_entity:\n",
    "    if n in heuristic_2_node_entity:\n",
    "        gt_entities = list(set(gt_node_entity[n]))\n",
    "        linked_entities = list(set(heuristic_2_node_entity[n]))\n",
    "        gt_entities.sort()\n",
    "        linked_entities.sort()\n",
    "        len_intersection = len(set(linked_entities).intersection(set(gt_entities)))\n",
    "        if len_intersection:\n",
    "            validated_nodes.add(n)\n",
    "\n",
    "print('Number of directly validated nodes:', len(validated_nodes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find indirect connection between gt entity and linked entity for unvalidated nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:30:37.321449Z",
     "start_time": "2021-01-04T16:30:37.316515Z"
    }
   },
   "outputs": [],
   "source": [
    "entity_nbrs = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:46:25.039698Z",
     "start_time": "2021-01-04T17:46:25.033072Z"
    }
   },
   "outputs": [],
   "source": [
    "# output file\n",
    "entity_nbrs = read_json(entity_nbrs_file, int_key=True, values_to_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:46:37.475381Z",
     "start_time": "2021-01-04T17:46:37.470900Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# **WARNING** if you don't have a GraphSense token, run the cell above (entity_nbrs_file)\n",
    "gt_entity_hop_nbrs = dict() # key: gt_entity, value: dict of key: hop, value: neighbors\n",
    "h = 1 # hop\n",
    "for n, es in gt_node_entity.items():\n",
    "    if n not in validated_nodes:\n",
    "        for e in es:\n",
    "            if e not in gt_entity_hop_nbrs:\n",
    "                gt_entity_hop_nbrs[e] = dict()\n",
    "                gt_entity_hop_nbrs[e][h] = set()\n",
    "            # get neighbors at hop 1\n",
    "            if not gt_entity_hop_nbrs[e][h] and e > 0: # TODO: check also negative entities (snakes, proxies, ...)\n",
    "                if e not in entity_nbrs:\n",
    "                    entity_nbrs[e] = get_entity_neighbors(e)\n",
    "                gt_entity_hop_nbrs[e][h] = entity_nbrs[e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:46:42.218588Z",
     "start_time": "2021-01-04T17:46:42.214404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701710153 1 03fce165537aea120bffe8505876b44d5119354f825b3eac329b761fc5636bf334\n",
      "702110475 1 0311cad0edf4ac67298805cf4407d94358ca60cd44f2e360856f3b1c088bcd4782\n"
     ]
    }
   ],
   "source": [
    "indirectly_validated_nodes = set()\n",
    "for n, es in gt_node_entity.items():\n",
    "    if n not in validated_nodes:\n",
    "        for e in es:\n",
    "            nbrs = gt_entity_hop_nbrs[e][h]\n",
    "            nbrs_linked_entities_intersection = nbrs.intersection(heuristic_2_node_entity[n])\n",
    "            if nbrs_linked_entities_intersection:\n",
    "                print(e, len(nbrs_linked_entities_intersection), n)\n",
    "                indirectly_validated_nodes.add(n)\n",
    "                validated_nodes.add(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:46:45.636161Z",
     "start_time": "2021-01-04T17:46:45.632372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of validated nodes: 9\n"
     ]
    }
   ],
   "source": [
    "print('Total number of validated nodes:', len(validated_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T11:36:28.760399Z",
     "start_time": "2020-10-05T11:36:28.756953Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "write_json(entity_nbrs, entity_nbrs_file, values_to_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:47:22.883257Z",
     "start_time": "2021-01-04T17:47:22.875576Z"
    }
   },
   "outputs": [],
   "source": [
    "# manually checked, spent before last_block = 647529\n",
    "settlement_txs_where_remote_spends_coins =[\n",
    "    'd6d658c4a13c8f2d2927a71e1cdd5ef310d7d9adb9f96774018276a3590c3788',\n",
    "    'be051cfb727c10c28c3975d8d32a0c29bb244b4f21bd14e2eab584219f496b27',\n",
    "    '9fa69e68dc5ce6525b3edeb7dba8f2d954adea3d82ca340a55b7157418d384c1',\n",
    "    '101c492db10266eb1c7cd63e00bcfbb9f60860a0badff6c7573b673f523b45f2',\n",
    "    'f65aef03e5a93d5acebf8135cd411ccf46013a957620eab2a5a95171327f4e93',\n",
    "    'e836d71d6cc8b5a79562b46890429a89ecc5e9e3be8cbc0203a1c00bd69c8d2a',\n",
    "    'fe9f60f930d1a7cad6b17923c8f3f041b5e2ba308447c73d82abc048389c930a',\n",
    "    '75f27715d27c6629673ddf080cf5267dc85bea40bf702fcb59377f82553b7e08',\n",
    "    '2894fac92b98402a993b6b57db0877db085d796978c89495b630871e12b2427f',\n",
    "    '4dd694546be280a08803b9e2eb9e15adfe0f4e4ef0f53567d9a7f183188ebcff',\n",
    "    '7e25d41fd47d10287e560c3d98cebe041c0f7ae57c1fff270ad662753ec706c0',\n",
    "    'f4c30c226bb4ce2c16673555768190318dd327bab9d06b51ff35a497483eff70',\n",
    "    '399cfcd171e69d7a0c150772bba5202850c0186e58d9b818cfaf6a4c74f567fa',\n",
    "    '05489ca075ff037934e734b893098559c10eabc953edca1cc2faa80fe1042582',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:47:40.284700Z",
     "start_time": "2021-01-04T17:47:40.280532Z"
    }
   },
   "outputs": [],
   "source": [
    "node_settlement_address_txs = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:47:40.691816Z",
     "start_time": "2021-01-04T17:47:40.687926Z"
    }
   },
   "outputs": [],
   "source": [
    "gt_address_txs = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:50:35.202185Z",
     "start_time": "2021-01-04T17:50:35.198806Z"
    }
   },
   "outputs": [],
   "source": [
    "# output file\n",
    "gt_address_txs = read_json(gt_address_txs_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:50:42.066390Z",
     "start_time": "2021-01-04T17:50:42.057071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes that received our coins and did not spend them: 41\n",
      "Number of addresses that received our coins and did not spend them 43\n"
     ]
    }
   ],
   "source": [
    "# **WARNING** if you don't have a GraphSense token, run the cell above (gt_address_txs_file)\n",
    "# check activity of node addresses not spending our coins\n",
    "nodes_that_spent_our_coins = set()\n",
    "for closed_channel in gt_outgoing_channels:\n",
    "    funding_hsh, out_index = closed_channel['channel_point'].split(':')\n",
    "    funded_address = funding_txs[funding_hsh]['vout'][int(out_index)]['scriptpubkey_address']\n",
    "    stx = funded_address_settlement_txs[funded_address][0]\n",
    "    # if there are two outputs, the first is remote and the second is local\n",
    "    if len(stx['vout']) == 2:\n",
    "        if stx['txid'] not in settlement_txs_where_remote_spends_coins:\n",
    "            if closed_channel['remote_pubkey'] not in node_settlement_address_txs:\n",
    "                node_settlement_address_txs[closed_channel['remote_pubkey']] = dict()\n",
    "            a = stx['vout'][0]['scriptpubkey_address']\n",
    "            if a not in node_settlement_address_txs[closed_channel['remote_pubkey']]:\n",
    "                if a not in gt_address_txs:\n",
    "                    gt_address_txs[a] = get_address_txs(a)\n",
    "                node_settlement_address_txs[closed_channel['remote_pubkey']][a] = gt_address_txs[a]\n",
    "            elif not node_settlement_address_txs[closed_channel['remote_pubkey']][a]:\n",
    "                if a not in address_txs:\n",
    "                    gt_address_txs[a] = get_address_txs(a)\n",
    "                node_settlement_address_txs[closed_channel['remote_pubkey']][a] = gt_address_txs[a]\n",
    "        else:\n",
    "            nodes_that_spent_our_coins.add(closed_channel['remote_pubkey'])\n",
    "for node in nodes_that_spent_our_coins:\n",
    "    node_settlement_address_txs.pop(node, None)\n",
    "\n",
    "print('Number of nodes that received our coins and did not spend them:', len(node_settlement_address_txs.keys()))\n",
    "print('Number of addresses that received our coins and did not spend them', sum([len(d.keys()) for d in node_settlement_address_txs.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T11:35:10.284225Z",
     "start_time": "2020-10-05T11:35:10.278232Z"
    }
   },
   "outputs": [],
   "source": [
    "write_json(gt_address_txs, gt_address_txs_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:51:18.092368Z",
     "start_time": "2021-01-04T17:51:18.081082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes that did not reuse addresses, received our coins and never spent any coins: 41\n"
     ]
    }
   ],
   "source": [
    "node_address_no_spend = set()\n",
    "# the node receives coins on addresses that have only 1 incoming tx\n",
    "for node, d in node_settlement_address_txs.items():\n",
    "    discard = False # discard if address is not new or spent coins\n",
    "    for a, txs in d.items():\n",
    "        # discard if the address has more than 1 incoming tx or spent \n",
    "        if txs['no_incoming_txs'] > 1 or txs['no_outgoing_txs']: \n",
    "            discard = True\n",
    "    if not discard:\n",
    "        node_address_no_spend.add(node)\n",
    "\n",
    "print('Number of nodes that did not reuse addresses, received our coins and never spent any coins:', len(node_address_no_spend))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:51:20.877527Z",
     "start_time": "2021-01-04T17:51:20.872117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes that received and spent our coins: 11\n",
      "Number of nodes that received and spent our coins and are validated: 7\n"
     ]
    }
   ],
   "source": [
    "received_spent_coins_nodes = set()\n",
    "for closed_channel in gt_outgoing_channels:\n",
    "    funding_hsh, out_index = closed_channel['channel_point'].split(':')\n",
    "    funded_address = funding_txs[funding_hsh]['vout'][int(out_index)]['scriptpubkey_address']\n",
    "    stx = funded_address_settlement_txs[funded_address][0]\n",
    "    # if there are two outputs, the first is remote and the second is local\n",
    "    if len(stx['vout']) == 2 and stx['txid'] in settlement_txs_where_remote_spends_coins:\n",
    "        received_spent_coins_nodes.add(closed_channel['remote_pubkey'])\n",
    "\n",
    "print('Number of nodes that received and spent our coins:', len(received_spent_coins_nodes))\n",
    "print('Number of nodes that received and spent our coins and are validated:', len(received_spent_coins_nodes.intersection(validated_nodes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:51:22.792837Z",
     "start_time": "2021-01-04T17:51:22.784122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes that received our coins and were linked with our heuristic: 52\n"
     ]
    }
   ],
   "source": [
    "received_coins_linked_nodes = received_coins_nodes.intersection(set(heuristic_2_node_entity.keys()))\n",
    "print('Number of nodes that received our coins and were linked with our heuristic:', len(received_coins_linked_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List info about non-validated nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:52:45.985986Z",
     "start_time": "2021-01-04T17:52:45.944073Z"
    }
   },
   "outputs": [],
   "source": [
    "nodes = pd.read_csv(nodes_csv_file)\n",
    "node_alias, alias_node = df_to_dicts_set(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:52:49.548270Z",
     "start_time": "2021-01-04T17:52:49.541291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03864ef025fde8fb587d989186ce6a4a186895ee44a926bfc370e2c366597a3f8f\n",
      "{'ACINQ'}\n",
      "{700855070}\n",
      "4600\n",
      "\n",
      "0303a518845db99994783f606e6629e705cfaf072e5ce9a4d8bf9e249de4fbd019\n",
      "{'LNBIG.com [lnd-25]'}\n",
      "{702145255}\n",
      "4600\n",
      "\n",
      "031ce29116eab7edd66148f5169f1fb658fad62bdc5091221ab895fe5d36db00b2\n",
      "{'LNBIG.com [lnd-05]'}\n",
      "{701940461}\n",
      "4600\n",
      "\n",
      "032d4baebebfdeab7a2ecef2fbe109cbef10de95f05aa54090fdb687789547dbf5\n",
      "{'CONNECT_WITH_ME'}\n",
      "{702410217}\n",
      "4600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in received_spent_coins_nodes:\n",
    "    if n not in validated_nodes:\n",
    "        print(n)\n",
    "        print(node_alias[n])\n",
    "        print(gt_node_entity[n])\n",
    "        print(len(heuristic_2_node_entity))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Compare with each Other\n",
    "Here we compare the linking results of heuristic 1 with the ones of heuristic 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:53:19.807516Z",
     "start_time": "2021-01-04T17:53:19.553719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On-chain heuristic used: none\n",
      "9042 entities out of 9042 entities in common are linked to the same nodes\n",
      "9042 entities out of 9042 entities in common are linked to at least on same node\n",
      "On-chain heuristic used: stars\n",
      "9042 entities out of 9042 entities in common are linked to the same nodes\n",
      "9042 entities out of 9042 entities in common are linked to at least on same node\n",
      "On-chain heuristic used: snakes\n",
      "9042 entities out of 9042 entities in common are linked to the same nodes\n",
      "9042 entities out of 9042 entities in common are linked to at least on same node\n",
      "On-chain heuristic used: collectors\n",
      "9193 entities out of 9193 entities in common are linked to the same nodes\n",
      "9193 entities out of 9193 entities in common are linked to at least on same node\n",
      "On-chain heuristic used: proxies\n",
      "10584 entities out of 10584 entities in common are linked to the same nodes\n",
      "10584 entities out of 10584 entities in common are linked to at least on same node\n",
      "On-chain heuristic used: all\n",
      "11272 entities out of 11272 entities in common are linked to the same nodes\n",
      "11272 entities out of 11272 entities in common are linked to at least on same node\n"
     ]
    }
   ],
   "source": [
    "heuristic_1_entity_node_dict = dict()\n",
    "heuristic_2_entity_node_dict = dict()\n",
    "for h in on_chain_heuristics_list:\n",
    "    heuristic_1_entity_node_dict[h] = read_json(heuristics_files[1][h][0] , True)\n",
    "\n",
    "for h in on_chain_heuristics_list:\n",
    "    heuristic_2_entity_node_dict[h] = read_json(heuristics_files[2][h][0], True)\n",
    "\n",
    "for h in on_chain_heuristics_list:\n",
    "    print('On-chain heuristic used:', h)\n",
    "    entities_heuristics_1_2 = set(heuristic_1_entity_node_dict[h]).intersection(set(heuristic_2_entity_node_dict[h]))\n",
    "    # to see if the two heuristics say the same\n",
    "    same = 0\n",
    "    intersect = 0\n",
    "    for e in entities_heuristics_1_2:\n",
    "        s1 = set(heuristic_1_entity_node_dict[h][e])\n",
    "        s2 = set(heuristic_2_entity_node_dict[h][e])\n",
    "        if s1 == s2:\n",
    "            same += 1\n",
    "            intersect += 1\n",
    "        elif s1.intersection(s2):\n",
    "            intersect += 1\n",
    "    print(same, 'entities out of', len(entities_heuristics_1_2),\n",
    "          'entities in common are linked to the same nodes')\n",
    "    print(intersect, 'entities out of', len(entities_heuristics_1_2),\n",
    "          'entities in common are linked to at least on same node')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Combining Linking and Alias-based heuristics\n",
    "\n",
    "In this section, we first increase the linking between entities and nodes using the alias/IP-based node clustering and then we deanonymize more nodes using known bitcoin addresses. \n",
    "\n",
    "#### Inputs (made available):\n",
    "- `address_categories_csv_file`\n",
    "- `funding_address_entity_file`\n",
    "- `settlement_address_entity_file`\n",
    "- `heuristics_files`\n",
    "- `nodes_csv_file`\n",
    "- `ips_csv_file`\n",
    "- `alias_address_clusters_csv_file`\n",
    "- `patterns_files`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T13:58:05.420159Z",
     "start_time": "2021-01-25T13:58:05.415155Z"
    }
   },
   "outputs": [],
   "source": [
    "alias_ip_cluster = pd.read_csv(alias_address_clusters_csv_file)\n",
    "node_entity = read_json(heuristics_files[2]['all'][1])\n",
    "ip_addresses = pd.read_csv(ips_csv_file)\n",
    "nodes = pd.read_csv(nodes_csv_file)\n",
    "known_addresses = pd.read_csv(address_categories_csv_file)\n",
    "funding_address_entity = read_json(funding_address_entity_file)\n",
    "settlement_address_entity = read_json(settlement_address_entity_file)\n",
    "entity_nodes = read_json(heuristics_files[2]['all'][0], int_key=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Augment Linking\n",
    "\n",
    "Here we combine the best-performing linking heuristic with the alias/IP-based clustering to increase the number of nodes linkable to Bitcoin entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:10:04.486770Z",
     "start_time": "2021-01-05T09:10:04.481198Z"
    }
   },
   "outputs": [],
   "source": [
    "node_cluster, cluster_node = df_to_dicts_set(alias_ip_cluster, invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:10:05.597494Z",
     "start_time": "2021-01-05T09:10:05.591105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# linkable nodes thanks to alias/IP-based clustering: 245\n"
     ]
    }
   ],
   "source": [
    "# count the number of nodes that are not directly linked to entities but are in an\n",
    "# alias/ip-based cluster of nodes where at least one node has been linked\n",
    "# to an entity\n",
    "\n",
    "linkable_nodes = set()\n",
    "for cluster, ns in cluster_node.items():\n",
    "    all_nodes_are_linked = False\n",
    "    possible_linkable_nodes = set()\n",
    "    for node in ns:\n",
    "        if node in node_entity:\n",
    "            all_nodes_are_linked = True\n",
    "        else:\n",
    "            # to better distinguish between what's already linked and not\n",
    "            possible_linkable_nodes.add(node)\n",
    "    if all_nodes_are_linked:\n",
    "        linkable_nodes = linkable_nodes.union(possible_linkable_nodes)\n",
    "print('# linkable nodes thanks to alias/IP-based clustering:', len(linkable_nodes)) # nNodesAliasIPLinkable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Augment Deanonymization\n",
    "Here we further deanonymize nodes that are linked to known Bitcoin addresses and aliases and IP addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:10:08.154525Z",
     "start_time": "2021-01-05T09:10:08.088408Z"
    }
   },
   "outputs": [],
   "source": [
    "node_alias, alias_node = df_to_dicts_set(nodes)\n",
    "node_ip, ip_node = df_to_dicts_set(ip_addresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:10:09.217112Z",
     "start_time": "2021-01-05T09:10:09.209123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# linked nodes with an alias 4284\n",
      "# linked nodes with an IP address 3919\n",
      "# nodes belonging to an alias/IP-based cluster 1251\n",
      "# linked nodes belonging to an alias/IP-based cluster 416\n"
     ]
    }
   ],
   "source": [
    "n_a = len(set(node_entity.keys()).intersection(set(node_alias.keys())))\n",
    "print('# linked nodes with an alias', n_a) # nNodesLinkedWithAlias\n",
    "\n",
    "n_ip = len(set(node_entity.keys()).intersection(set(node_ip.keys())))\n",
    "print('# linked nodes with an IP address', n_ip) # nNodesLinkedWithIP\n",
    "\n",
    "print('# nodes belonging to an alias/IP-based cluster', len(node_cluster)) # nNodesAliasIPCluster\n",
    "intersection = set(node_entity.keys()).intersection(set(alias_ip_cluster.pub_key.values))\n",
    "n_c = len(intersection)\n",
    "print('# linked nodes belonging to an alias/IP-based cluster', n_c) # nNodesLinkedAliasIPCluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:10:11.948598Z",
     "start_time": "2021-01-05T09:10:11.941871Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_tagged_addresses(tagged_entities):\n",
    "    tagged_addresses = set()\n",
    "    for address_entity in [funding_address_entity, settlement_address_entity]:\n",
    "        for address, entity in address_entity.items():\n",
    "            component = 0\n",
    "            for entity_component in [pattern_double_mapping[pattern][0] for pattern in patterns_list]:\n",
    "                if entity in entity_component:\n",
    "                    component = entity_component[entity]\n",
    "            if not component:\n",
    "                component = - entity\n",
    "            if - component in tagged_entities:\n",
    "                tagged_addresses.add(address)\n",
    "    return tagged_addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:10:12.654497Z",
     "start_time": "2021-01-05T09:10:12.603026Z"
    }
   },
   "outputs": [],
   "source": [
    "pattern_double_mapping = dict() # entity-star, star-entity\n",
    "for pattern in patterns_list:\n",
    "    pattern_double_mapping[pattern] = df_to_two_dicts(pd.read_csv(patterns_files[pattern])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:10:17.319080Z",
     "start_time": "2021-01-05T09:10:14.247639Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# linked entities with an alias or IP address 17260\n",
      "# linked entities with an alias 17135\n",
      "# linked entities with an IP address 16545\n"
     ]
    }
   ],
   "source": [
    "tagged_entities = set()  # actually this should be tagged_components\n",
    "alias_tagged_entities = set()\n",
    "ip_tagged_entities = set()\n",
    "\n",
    "for node in node_alias:\n",
    "    if node in node_entity:\n",
    "        tagged_entities = tagged_entities.union(node_entity[node])\n",
    "        alias_tagged_entities = alias_tagged_entities.union(node_entity[node])\n",
    "for node in node_ip:\n",
    "    if node in node_entity:\n",
    "        tagged_entities = tagged_entities.union(node_entity[node])\n",
    "        ip_tagged_entities = ip_tagged_entities.union(node_entity[node])\n",
    "\n",
    "print('# linked entities with an alias or IP address', len(tagged_entities)) # nEntitiesAliasIPTagged\n",
    "print('# linked entities with an alias', len(alias_tagged_entities))\n",
    "print('# linked entities with an IP address', len(ip_tagged_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:10:20.983019Z",
     "start_time": "2021-01-05T09:10:20.344417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# linked addresses with an alias or IP address 52056\n",
      "# linked addresses with an alias 51747\n",
      "# linked addresses with an IP address 50275\n"
     ]
    }
   ],
   "source": [
    "tagged_addresses = compute_tagged_addresses(tagged_entities)\n",
    "alias_tagged_addresses = compute_tagged_addresses(alias_tagged_entities)\n",
    "ip_tagged_addresses = compute_tagged_addresses(ip_tagged_entities)\n",
    "\n",
    "print('# linked addresses with an alias or IP address', len(tagged_addresses)) # nAddressesAliasIPTagged\n",
    "print('# linked addresses with an alias', len(alias_tagged_addresses)) # nAddressesAliasTagged\n",
    "print('# linked addresses with an IP address', len(ip_tagged_addresses)) # nAddressesIPTagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:10:27.706193Z",
     "start_time": "2021-01-05T09:10:27.522093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use stars\n",
      "use snakes\n",
      "use collectors\n",
      "use proxies\n"
     ]
    }
   ],
   "source": [
    "on_chain_heuristics = {och: (True if och != 'none' else False) for och in on_chain_heuristics_list}\n",
    "funding_address_entity, settlement_address_entity, = set_mapping(funding_address_entity, settlement_address_entity, on_chain_heuristics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:10:30.471204Z",
     "start_time": "2021-01-05T09:10:30.416270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of known addresses linked to nodes 1\n",
      "number of nodes linked to known addresses 1\n",
      "{'03021c5f5f57322740e4'}\n"
     ]
    }
   ],
   "source": [
    "known_addresses_nodes = dict()\n",
    "linked_nodes = set()\n",
    "for address in known_addresses.address:\n",
    "    if address in funding_address_entity:\n",
    "        entity = funding_address_entity[address]\n",
    "        if entity in entity_nodes:\n",
    "            nodes = entity_nodes[entity]\n",
    "            # print(address, entity, nodes)\n",
    "            known_addresses_nodes[address] = nodes\n",
    "            linked_nodes = linked_nodes.union(nodes)\n",
    "    elif address in settlement_address_entity:\n",
    "        entity = settlement_address_entity[address]\n",
    "        if entity in entity_nodes:\n",
    "            nodes = entity_nodes[entity]\n",
    "            # print(address, entity, nodes)\n",
    "            known_addresses_nodes[address] = nodes\n",
    "            linked_nodes = linked_nodes.union(nodes)\n",
    "\n",
    "print('number of known addresses linked to nodes', len(known_addresses_nodes))\n",
    "print('number of nodes linked to known addresses', len(linked_nodes))\n",
    "for node in linked_nodes:\n",
    "    if node in node_alias:\n",
    "        print(node_alias[node])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...not much!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra\n",
    "\n",
    "In this section, we link channels to the nodes that funded them\n",
    "\n",
    "#### Inputs (made available):\n",
    "- `funding_address_entity_file`\n",
    "- `settlement_address_entity_file`\n",
    "- `heuristics_files`\n",
    "- `channels_file`\n",
    "- `funding_txs_file`\n",
    "\n",
    "#### Outputs (made available):\n",
    "- `funding_node_channel_csv_file`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T15:28:17.021468Z",
     "start_time": "2021-01-13T15:28:14.346339Z"
    }
   },
   "outputs": [],
   "source": [
    "funding_address_entity = read_json(funding_address_entity_file)\n",
    "settlement_address_entity = read_json(settlement_address_entity_file)\n",
    "entity_nodes = read_json(heuristics_files[2]['all'][0], int_key=True)\n",
    "channels_df = pd.read_csv(channels_file)\n",
    "funding_txs = read_json(funding_txs_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T15:28:26.031315Z",
     "start_time": "2021-01-13T15:28:26.009259Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1002, -3461, -3160, 507901077, -558, -2540, -568, -1865, -2711, -781]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(entity_nodes.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T15:28:30.092389Z",
     "start_time": "2021-01-13T15:28:30.080137Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e for e in funding_address_entity.values() if e < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T15:28:33.417029Z",
     "start_time": "2021-01-13T15:28:33.245740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use stars\n",
      "use snakes\n",
      "use collectors\n",
      "use proxies\n"
     ]
    }
   ],
   "source": [
    "on_chain_heuristics = {och: (True if och != 'none' else False) for och in on_chain_heuristics_list}\n",
    "funding_address_entity, settlement_address_entity, = set_mapping(funding_address_entity, settlement_address_entity, on_chain_heuristics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T15:28:41.148008Z",
     "start_time": "2021-01-13T15:28:41.144840Z"
    }
   },
   "outputs": [],
   "source": [
    "# [e for e in funding_address_entity.values() if e < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T15:28:43.374991Z",
     "start_time": "2021-01-13T15:28:42.714188Z"
    }
   },
   "outputs": [],
   "source": [
    "funding_node_channel = []\n",
    "for channel in channels_df.values:\n",
    "    funding_tx, out_index = channel[0].split(':')\n",
    "    n1, n2 = channel[1], channel[2]\n",
    "    address = funding_txs[funding_tx]['vin'][0]['prevout']['scriptpubkey_address']\n",
    "    funding_entity = funding_address_entity[address]\n",
    "    # if its funding entity is linked to a node\n",
    "    if funding_entity in entity_nodes:\n",
    "        # if only one node in the channel is linked to the funding entity\n",
    "        to_assign = []\n",
    "        if n1 in entity_nodes[funding_entity]:\n",
    "            to_assign.append(n1)\n",
    "        if n2 in entity_nodes[funding_entity]:\n",
    "            to_assign.append(n1)\n",
    "        if len(to_assign) == 1:\n",
    "            funding_node_channel.append([to_assign[0], channel[0]])\n",
    "\n",
    "columns = ['funding_node', 'channel']\n",
    "funding_node_channel_df = pd.DataFrame(funding_node_channel, columns=columns)\n",
    "funding_node_channel_df.to_csv(funding_node_channel_csv_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
