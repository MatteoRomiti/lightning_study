{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T17:09:51.550760Z",
     "start_time": "2020-09-30T17:09:51.548241Z"
    }
   },
   "source": [
    "# On-Chain Data Collection\n",
    "In this notebook, we take care of funding and settlement transactions and addresses. The sections are:\n",
    "1. Funding Side\n",
    "    - 1.1 Fetch Funding Txs \n",
    "    - 1.2 Clean Funding Txs \n",
    "    - 1.3 Funding Addresses \n",
    "    - 1.4 Store Data \n",
    "2. Settlement Side\n",
    "    - 2.1 Fetch Settlement Txs \n",
    "    - 2.2 Clean Settlement Txs \n",
    "    - 2.3 Settlement Addresses \n",
    "    - 2.4 Store Data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T15:35:27.749120Z",
     "start_time": "2021-01-30T15:35:26.878511Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utils import read_json, write_json, get_blockstream_tx, get_blockchain_page, wasabi_addresses, get_available_txs\n",
    "\n",
    "# input files\n",
    "from utils import channels_file, samourai_txs_file\n",
    "\n",
    "# output files\n",
    "from utils import funding_txs_file, funding_addresses_file, funding_addresses_csv_file, unilateral_settlement_txs_with_p2wsh_in_outputs_spenders_file, spender_details_file, settlement_txs_with_punishment_file, funded_address_settlement_txs_file, settlement_txs_file, settlement_addresses_file, settlement_addresses_csv_file\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Funding Side\n",
    "\n",
    "In this section, we fetch, clean and store funding txs and funding addresses of the channels in our dataset.\n",
    "\n",
    "#### Inputs (made available):\n",
    "- `channels_file`\n",
    "- `samourai_txs_file`\n",
    "\n",
    "#### Outputs (made available):\n",
    "- `funding_txs_file`\n",
    "- `funding_addresses_file`\n",
    "- `funding_addresses_csv_file`\n",
    "\n",
    "You can either run all the cells (some might take several minutes to complete, look for the \\*\\*WARNING** comment in the cells) or run directly the \"output files\" cell and have a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T15:35:33.299199Z",
     "start_time": "2021-01-30T15:35:32.598055Z"
    }
   },
   "outputs": [],
   "source": [
    "# input files\n",
    "channels_df = pd.read_csv(channels_file) # from step #0\n",
    "samourai_txs = set(read_json(samourai_txs_file)) # from https://github.com/nopara73/WasabiVsSamourai/tree/master/WasabiVsSamourai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T15:35:39.726168Z",
     "start_time": "2021-01-30T15:35:36.943572Z"
    }
   },
   "outputs": [],
   "source": [
    "# output files (the results of section 1. Funding Side)\n",
    "funding_txs = read_json(funding_txs_file)\n",
    "funding_addresses = read_json(funding_addresses_file)\n",
    "# funding_addresses_df = pd.read_csv(funding_addresses_csv_file) # if you prefer pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T12:01:15.729285Z",
     "start_time": "2020-09-30T12:01:15.726894Z"
    }
   },
   "source": [
    "## 1.1 Fetch Funding Txs\n",
    "Here you find the code to fetch the funding txs from Blockstream API, starting from a list of channel points. Not needed if you already read the output files above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T13:30:02.449612Z",
     "start_time": "2021-01-04T13:30:02.274690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of funding txs: 98240\n"
     ]
    }
   ],
   "source": [
    "# initialize dict\n",
    "funding_txs = dict()\n",
    "for channel in channels_df.chan_point.values:\n",
    "    hsh = channel.split(':')[0]\n",
    "    if hsh not in funding_txs:\n",
    "        funding_txs[hsh] = None\n",
    "print('Number of funding txs:', len(funding_txs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T09:05:43.629873Z",
     "start_time": "2020-09-30T09:05:43.623341Z"
    }
   },
   "outputs": [],
   "source": [
    "# (re)run until all txs are fetched\n",
    "# **WARNING**, it might take some minutes\n",
    "for i, hsh in enumerate(funding_txs):\n",
    "    if not isinstance(funding_txs[hsh], dict):\n",
    "        print('Fetching funding tx', i, end='\\r')\n",
    "        funding_txs[hsh] = get_blockstream_tx(hsh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Clean Funding Txs\n",
    "Here you find the code to:\n",
    "- remove txs not in GraphSense (available till `last_block`)\n",
    "- remove coinjoins (wasabi and samourai)\n",
    "\n",
    "Not needed if you already read the output files above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T10:55:52.112521Z",
     "start_time": "2020-09-30T10:55:52.037816Z"
    }
   },
   "outputs": [],
   "source": [
    "funding_txs = get_available_txs(funding_txs) # Remove txs not in GraphSense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T10:56:32.030705Z",
     "start_time": "2020-09-30T10:56:31.227309Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove coinjoins (wasabi and samourai)\n",
    "wasabi_coinjoins = set()\n",
    "for hsh, tx in funding_txs.items():\n",
    "    for vout in tx['vout']:\n",
    "        if vout['scriptpubkey_address'] in wasabi_addresses:\n",
    "            wasabi_coinjoins.add(hsh)\n",
    "            \n",
    "coinjoin_free_funding_txs = dict()\n",
    "for hsh, tx in funding_txs.items():\n",
    "    if hsh not in wasabi_coinjoins and hsh not in samourai_txs:\n",
    "        coinjoin_free_funding_txs[hsh] = tx\n",
    "funding_txs = coinjoin_free_funding_txs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T15:23:30.834004Z",
     "start_time": "2020-09-28T15:23:30.831496Z"
    }
   },
   "source": [
    "## 1.3 Funding Addresses\n",
    "Here you find the code to compute and save the set of funding addresses.\n",
    "Not needed if you already read the output files above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T10:56:58.596561Z",
     "start_time": "2020-09-30T10:56:58.464807Z"
    }
   },
   "outputs": [],
   "source": [
    "funding_addresses = set()\n",
    "for hsh, tx in funding_txs.items():\n",
    "    for vin in tx['vin']:\n",
    "        funding_addresses.add(vin['prevout']['scriptpubkey_address'])\n",
    "funding_addresses = list(funding_addresses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Store Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T10:57:09.931730Z",
     "start_time": "2020-09-30T10:57:09.929037Z"
    }
   },
   "outputs": [],
   "source": [
    "write_json(funding_txs, funding_txs_file)\n",
    "write_json(funding_addresses, funding_addresses_file)\n",
    "pd.DataFrame(funding_addresses, columns='address').to_csv(funding_addresses_csv_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Settlement Side\n",
    "\n",
    "In this section, we fetch, clean and store funded addresses (multisig addresses), settlement txs and settlement addresses of the channels in our dataset.\n",
    "\n",
    "#### Inputs (made available):\n",
    "- `channels_file`\n",
    "\n",
    "#### Outputs (made available):\n",
    "- `funded_address_settlement_txs_file`\n",
    "- `settlement_txs_file`\n",
    "- `settlement_addresses_file`\n",
    "\n",
    "You can either run all the cells (some might take several minutes to complete, look for the \\*\\*WARNING** comment in the cells) or run directly the \"output files\" cell and have a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T15:35:56.986564Z",
     "start_time": "2021-01-30T15:35:56.743791Z"
    }
   },
   "outputs": [],
   "source": [
    "# input files\n",
    "channels_df = pd.read_csv(channels_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T15:36:05.456171Z",
     "start_time": "2021-01-30T15:36:02.078832Z"
    }
   },
   "outputs": [],
   "source": [
    "# output files (the results of section 2. Settlement Side)\n",
    "funded_address_settlement_txs = read_json(funded_address_settlement_txs_file)\n",
    "settlement_txs = read_json(settlement_txs_file)\n",
    "settlement_addresses = read_json(settlement_addresses_file)\n",
    "# settlement_addresses_df = pd.read_csv(settlement_addresses_csv_file) # if you prefer pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Fetch Settlement Txs\n",
    "Here you find the code to fetch the settlement txs from Blockstream API, starting from a list of channel points. Not needed if you already read the output files above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T11:01:09.645321Z",
     "start_time": "2020-09-30T11:01:09.424409Z"
    }
   },
   "outputs": [],
   "source": [
    "# initialize dict\n",
    "funded_address_settlement_txs = dict()\n",
    "for i, channel in enumerate(channels_df.chan_point.values):\n",
    "    hsh, out_index = channel.split(':')\n",
    "    funded_address = funding_txs[hsh]['vout'][int(out_index)]['scriptpubkey_address']        \n",
    "    if funded_address not in funded_address_settlement_txs:\n",
    "        funded_address_settlement_txs[funded_address] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T11:01:11.305825Z",
     "start_time": "2020-09-30T11:01:11.303366Z"
    }
   },
   "outputs": [],
   "source": [
    "max_reached = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T11:01:18.873911Z",
     "start_time": "2020-09-30T11:01:18.868801Z"
    }
   },
   "outputs": [],
   "source": [
    "# (re)run until all settlement txs are fetched\n",
    "# **WARNING**, it might take some minutes\n",
    "for i, funded_address in enumerate(funded_address_settlement_txs):\n",
    "    if i >= max_reached:\n",
    "        if not isinstance(funded_address_settlement_txs[funded_address], list) or not funded_address_settlement_txs[funded_address]:\n",
    "            print(i, end='\\r')\n",
    "            txs = get_blockstream_address_txs(funded_address)\n",
    "            if len(txs) > 2:\n",
    "                # doesn't happen in our dataset\n",
    "                print('Address with > 2 settlement txs:', len(txs), funded_address)\n",
    "            for tx in txs:\n",
    "                if funded_address in [e['prevout']['scriptpubkey_address'] for e in tx['vin']]:\n",
    "                    # funded address is in inputs \n",
    "                    funded_address_settlement_txs[funded_address] = []\n",
    "                    funded_address_settlement_txs[funded_address].append(tx)\n",
    "        max_reached += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T11:01:33.014197Z",
     "start_time": "2020-09-30T11:01:33.011678Z"
    }
   },
   "outputs": [],
   "source": [
    "settlement_txs = dict()\n",
    "for stxs in funded_address_settlement_txs.values():\n",
    "    for stx in stxs:\n",
    "        settlement_txs[stx['txid']] = stx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Clean Settlement Txs\n",
    "Here you find the code to:\n",
    "- remove txs not in GraphSense (available till `last_block`)\n",
    "- remove txs involved in punishment txs\n",
    "\n",
    "Not needed if you already read the output files above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Remove txs not in GraphSense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T11:02:51.406130Z",
     "start_time": "2020-09-30T11:02:51.363769Z"
    }
   },
   "outputs": [],
   "source": [
    "settlement_txs = get_available_txs(settlement_txs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T15:20:12.399860Z",
     "start_time": "2020-09-28T15:20:12.397481Z"
    }
   },
   "source": [
    "### 2.2.2 Remove txs involved in punishment txs\n",
    "\n",
    "Here you find the code to find out with settlement txs were followed by a punishment tx (details [here](https://github.com/lightningnetwork/lightning-rfc/blob/master/03-transactions.md)).\n",
    "\n",
    "The procedure is as follows:\n",
    "1. select all settlement txs that:\n",
    "    - have two outputs and\n",
    "    - have at least one `p2wsh` output\n",
    "2. for these selected txs:\n",
    "    - get the spending tx of each `p2wsh` output\n",
    "    - check that the witness of the related input has length 3 and its second element is not null\n",
    "    \n",
    "In the following steps, additional intermediate files are generated, but we also provide them in case you don't want to run all the cells.\n",
    "\n",
    "Not needed if you already read the output files above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T15:36:17.396812Z",
     "start_time": "2021-01-30T15:36:17.227762Z"
    }
   },
   "outputs": [],
   "source": [
    "# intermediate files (generated in the following steps)\n",
    "\n",
    "unilateral_settlement_txs_with_p2wsh_in_outputs_spenders = read_json(unilateral_settlement_txs_with_p2wsh_in_outputs_spenders_file)\n",
    "spender_details = read_json(spender_details_file)\n",
    "settlement_txs_with_punishment = read_json(settlement_txs_with_punishment_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T11:04:20.521350Z",
     "start_time": "2020-09-30T11:04:20.476283Z"
    }
   },
   "outputs": [],
   "source": [
    "possible_two_output_unilateral_settlement_txs = dict()\n",
    "for tx in settlement_txs.values():\n",
    "    if 'vout' in tx and len(tx['vout']) == 2:\n",
    "        possible_two_output_unilateral_settlement_txs[tx['txid']] = settlement_txs[tx['txid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T11:07:26.756375Z",
     "start_time": "2020-09-30T11:07:26.719874Z"
    }
   },
   "outputs": [],
   "source": [
    "p2wsh_outputs = set() # tx_hash:out_index\n",
    "unilateral_settlement_txs_with_p2wsh_in_outputs = []\n",
    "for t in possible_two_output_unilateral_settlement_txs.values():\n",
    "    seq = t['vin'][0]['sequence']\n",
    "    if seq != 4294967295:\n",
    "        # unilateral\n",
    "        n_p2wsh_in_tx = 0\n",
    "        for i, vout in enumerate(t['vout']):\n",
    "            if vout['scriptpubkey_type'] == 'v0_p2wsh':\n",
    "                p2wsh_outputs.add(t['txid'] + ':' + str(i))\n",
    "                n_p2wsh_in_tx += 1\n",
    "        if n_p2wsh_in_tx:\n",
    "            unilateral_settlement_txs_with_p2wsh_in_outputs.append(t['txid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T11:07:29.253321Z",
     "start_time": "2020-09-30T11:07:29.248447Z"
    }
   },
   "outputs": [],
   "source": [
    "hash_page = dict()\n",
    "for tx_hash in unilateral_settlement_txs_with_p2wsh_in_outputs:\n",
    "    hash_page[tx_hash] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T11:07:38.975739Z",
     "start_time": "2020-09-30T11:07:38.972349Z"
    }
   },
   "outputs": [],
   "source": [
    "# **WARNING**, it might take some minutes\n",
    "for i, hsh in enumerate(hash_page):\n",
    "    print(i, end='\\r')\n",
    "    if not hash_page[hsh]:\n",
    "        # better source here: https://github.com/Blockstream/esplora/blob/master/API.md#get-txtxidoutspendvout\n",
    "        hash_page[hsh] = get_blockchain_page(hsh) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T11:10:10.167458Z",
     "start_time": "2020-09-30T11:10:10.164361Z"
    }
   },
   "outputs": [],
   "source": [
    "# not needed if you have already read the intermediate files above \n",
    "unilateral_settlement_txs_with_p2wsh_in_outputs_spenders = dict()\n",
    "for hsh in hash_page:\n",
    "    spenders = []\n",
    "    for i in [1, 2]:\n",
    "        # spender = hash_page[tx_hash].split('spender')[i].split(\"\\\"\")[4]\n",
    "        spender = hash_page[hsh].split('spender')[i].split(\"\\\"\")[6]\n",
    "        if len(spender) == 64:\n",
    "            spenders.append(spender)\n",
    "    unilateral_settlement_txs_with_p2wsh_in_outputs_spenders[hsh] = spenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T11:35:39.876393Z",
     "start_time": "2020-09-30T11:35:39.854980Z"
    }
   },
   "outputs": [],
   "source": [
    "# not needed if you have already read the intermediate files above\n",
    "spender_details = dict()\n",
    "for stx in unilateral_settlement_txs_with_p2wsh_in_outputs_spenders:\n",
    "    spenders = unilateral_settlement_txs_with_p2wsh_in_outputs_spenders[stx]\n",
    "    for i, spender in enumerate(spenders): # two spender txs at most\n",
    "        stx_out_index = stx + ':' + str(i)\n",
    "        if stx_out_index in p2wsh_outputs and spender not in spender_details:\n",
    "            spender_details[spender] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T11:36:28.188304Z",
     "start_time": "2020-09-30T11:36:28.185860Z"
    }
   },
   "outputs": [],
   "source": [
    "# not needed if you have already read the intermediate files above\n",
    "# **WARNING**, it might take some minutes\n",
    "for i, tx in enumerate(spender_details):\n",
    "    print(i, end='\\r')\n",
    "    if not spender_details[tx]:\n",
    "        details = get_blockstream_tx(tx)\n",
    "        spender_details[tx] = details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T11:37:11.750642Z",
     "start_time": "2020-09-30T11:37:11.709530Z"
    }
   },
   "outputs": [],
   "source": [
    "# for each spender I need not only the tx hash,\n",
    "# but also the address and the amount to be more secure\n",
    "spender_address_btc = set()  # spender tx hash + address + btc\n",
    "for stx in unilateral_settlement_txs_with_p2wsh_in_outputs_spenders:\n",
    "    spenders = unilateral_settlement_txs_with_p2wsh_in_outputs_spenders[stx]\n",
    "    for i, spender in enumerate(spenders):  # two spender txs at most\n",
    "        stx_out_index = stx + ':' + str(i)\n",
    "        if stx_out_index in p2wsh_outputs:\n",
    "            address = settlement_txs[stx]['vout'][i]['scriptpubkey_address']  # the output address of the settlement tx\n",
    "            btc = settlement_txs[stx]['vout'][i]['value']\n",
    "            key = spender + ':' + address + ':' + str(btc)\n",
    "            spender_address_btc.add(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T11:37:19.214639Z",
     "start_time": "2020-09-30T11:37:19.183068Z"
    }
   },
   "outputs": [],
   "source": [
    "# the witness of the input has length 3 and the second element is not null\n",
    "non_collaborative_settlement_address_btc = dict()\n",
    "non_standard_witnesses = []\n",
    "n_punishment_txs = 0\n",
    "non_standard_witness_len = dict()\n",
    "for sab in spender_address_btc:\n",
    "    spender, address, btc = sab.split(':')\n",
    "    details = spender_details[spender]  # cannot use spender_address_btc_details cause it has more keys\n",
    "    for vin in details['vin']:\n",
    "        if vin['prevout']['scriptpubkey_address'] == address and vin['prevout']['value'] == int(btc):\n",
    "            len_witness = len(vin['witness'])\n",
    "            if len_witness != 3:\n",
    "                if len_witness not in non_standard_witness_len:\n",
    "                    non_standard_witness_len[len_witness] = 0\n",
    "                non_standard_witness_len[len_witness] += 1\n",
    "                # print(details['txid'], 'non standard witness')\n",
    "                non_standard_witnesses.append(vin['witness'])\n",
    "#                 print(len(vin['witness']), end=' ')\n",
    "            else:\n",
    "                x = vin['witness'][1]\n",
    "                if x:\n",
    "                    # print(spender, x)\n",
    "                    n_punishment_txs += 1\n",
    "                    key = address + ':' + btc\n",
    "                    if key not in non_collaborative_settlement_address_btc:\n",
    "                        non_collaborative_settlement_address_btc[key] = 0\n",
    "                    non_collaborative_settlement_address_btc[key] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T11:37:25.448411Z",
     "start_time": "2020-09-30T11:37:25.406812Z"
    }
   },
   "outputs": [],
   "source": [
    "# not needed if you have already read the intermediate files above\n",
    "settlement_txs_with_punishment = set()\n",
    "# for each settlement tx\n",
    "for tx in possible_two_output_unilateral_settlement_txs:\n",
    "    # if one output has a p2wsh and the address and the value are in non_collaborative_settlement_address_btc\n",
    "        # add settlement tx to unusable\n",
    "    for vout in possible_two_output_unilateral_settlement_txs[tx]['vout']:\n",
    "        if vout['scriptpubkey_type'] == 'v0_p2wsh':\n",
    "            key = vout['scriptpubkey_address'] + ':' + str(vout['value'])\n",
    "            if key in non_collaborative_settlement_address_btc:\n",
    "                settlement_txs_with_punishment.add(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T11:37:27.714275Z",
     "start_time": "2020-09-30T11:37:27.710269Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(settlement_txs_with_punishment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T11:39:27.496580Z",
     "start_time": "2020-09-30T11:39:27.332622Z"
    }
   },
   "outputs": [],
   "source": [
    "non_punishment_funded_address_settlement_txs = dict()\n",
    "for fa, stxs in funded_address_settlement_txs.items():\n",
    "    non_punishment_funded_address_settlement_txs[fa] = []\n",
    "    for stx in stxs:\n",
    "        if stx['txid'] not in settlement_txs_with_punishment:\n",
    "            non_punishment_funded_address_settlement_txs[fa].append(stx)\n",
    "\n",
    "settlement_txs = dict()\n",
    "for stxs in non_punishment_funded_address_settlement_txs.values():\n",
    "    for stx in stxs:\n",
    "        settlement_txs[stx['txid']] = stx\n",
    "funded_address_settlement_txs = non_punishment_funded_address_settlement_txs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Settlement Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T17:19:37.601742Z",
     "start_time": "2020-09-30T17:19:37.525175Z"
    }
   },
   "outputs": [],
   "source": [
    "settlement_addresses = set()\n",
    "for fa, stxs in funded_address_settlement_txs.items():\n",
    "    for tx in stxs:\n",
    "        for vout in tx['vout']:\n",
    "            settlement_addresses.add(vout['scriptpubkey_address'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Store Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T17:13:11.176234Z",
     "start_time": "2020-09-30T17:13:11.173883Z"
    }
   },
   "outputs": [],
   "source": [
    "write_json(unilateral_settlement_txs_with_p2wsh_in_outputs_spenders, unilateral_settlement_txs_with_p2wsh_in_outputs_spenders_file)\n",
    "write_json(spender_details, spender_details_file)\n",
    "write_json(list(settlement_txs_with_punishment), settlement_txs_with_punishment_file)\n",
    "write_json(funded_address_settlement_txs, funded_address_settlement_txs_file)\n",
    "write_json(settlement_txs, settlement_txs_file)\n",
    "write_json(settlement_addresses, settlement_addresses_file)\n",
    "pd.DataFrame(settlement_addresses, columns='address').to_csv(settlement_addresses_csv_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
