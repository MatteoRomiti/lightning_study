{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Off-Chain Clustering\n",
    "In this notebook, we show you how we cluster LN nodes based on their alias and IP address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T08:17:44.768727Z",
     "start_time": "2020-10-06T08:17:42.757509Z"
    }
   },
   "outputs": [],
   "source": [
    "from jellyfish import levenshtein_distance, damerau_levenshtein_distance, \\\n",
    "    hamming_distance\n",
    "from matplotlib import rcParams\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # progress bars\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from plotnine import theme_bw, theme, element_text, ggplot, geom_bar, aes, \\\n",
    "    geom_text, labs\n",
    "\n",
    "# input files\n",
    "from utils import nodes_csv_file, ips_csv_file, whois_csv_file, \\\n",
    "    get_same_asn_clusters, evaluate_single_result, evaluate_measure\n",
    "\n",
    "# output files\n",
    "from utils import alias_address_clusters_csv_file\n",
    "\n",
    "from utils import relative_lcs, cluster, lcs_distance, \\\n",
    "    relative_levenshtein, relative_damerau_levenshtein, relative_hamming, \\\n",
    "    jaro_distance, jaro_winkler_distance, is_reserved_address\n",
    "\n",
    "theme_publication = theme_bw() + theme(text=element_text(family=\"cmr10\", size=12, color=\"black\"), axis_title=element_text(size=14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T08:17:46.389131Z",
     "start_time": "2020-10-06T08:17:46.311518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 9724 aliases\n"
     ]
    }
   ],
   "source": [
    "node_aliases = pd.read_csv(nodes_csv_file)\n",
    "# have each pub_key / alias combination in a separate row\n",
    "node_aliases.alias = node_aliases.alias.apply(lambda x: x.split(\"|\"))\n",
    "node_aliases = node_aliases.explode(\"alias\")\n",
    "# filter to remove empty aliases\n",
    "node_aliases = node_aliases[node_aliases.alias.str.len() > 0].copy()\n",
    "print(\"Got\", len(node_aliases), \"aliases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T08:17:50.184304Z",
     "start_time": "2020-10-06T08:17:48.179471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing a total of:  393 reserved IPs\n"
     ]
    }
   ],
   "source": [
    "# Load IP addresses\n",
    "node_ips = pd.read_csv(ips_csv_file)\n",
    "# have each pub_key / ip combination in a separate row\n",
    "node_ips.ip_address = node_ips.ip_address.apply(lambda x: x.split(\"|\"))\n",
    "node_ips = node_ips.explode(\"ip_address\")\n",
    "# separate port and ip address for easier querying\n",
    "node_ips['port'] = node_ips.ip_address.apply(lambda x: x.rsplit(\":\", 1)[1])\n",
    "node_ips['ip_address'] = node_ips.ip_address.apply(lambda x: x.rsplit(\":\", 1)[0].strip(\"[]\"))\n",
    "\n",
    "# FILTER (remove addresses that don't make sense)\n",
    "# https://en.wikipedia.org/wiki/Reserved_IP_addresses\n",
    "\n",
    "reserved_ips = node_ips.ip_address.apply(is_reserved_address)\n",
    "print(\"Removing a total of:\", sum(reserved_ips), \"reserved IPs\")\n",
    "node_ips = node_ips[~reserved_ips]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T08:19:30.653640Z",
     "start_time": "2020-10-06T08:19:30.485070Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    whois_data = pd.read_csv(whois_csv_file)\n",
    "    whois_data = whois_data[~whois_data.entities.isna()]\n",
    "except:\n",
    "    print(\"No existing whois data, querying all IP addresses now\")\n",
    "    from ipwhois import IPWhois\n",
    "\n",
    "    def lookup(ip_address):\n",
    "        fail = {\"query\": ip_address}\n",
    "        if \".onion\" in ip_address:\n",
    "            return fail\n",
    "        else:\n",
    "            try:\n",
    "                res = IPWhois(ip_address).lookup_rdap(depth=1)\n",
    "                return res\n",
    "            except:\n",
    "                print(ip_address, \"couldn't be queried...\")\n",
    "                return fail\n",
    "    whois_jsons = [lookup(ip_address) for ip_address in tqdm(node_ips.ip_address.unique())]\n",
    "    whois_data = pd.DataFrame(whois_jsons)\n",
    "    # whois_data = pd.DataFrame.from_dict(whois_jsons) # tofix\n",
    "    whois_data.to_csv(whois_csv_file, index=False)\n",
    "\n",
    "# only keep asn and ip_address\n",
    "whois_data = whois_data[[\"asn\", \"query\"]].rename(columns={\"query\": \"ip_address\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_aliases = node_aliases[node_aliases.alias.str.contains(\"LNBIG\")]\n",
    "# add random \"Lightning\" aliases\n",
    "tmp_aliases = tmp_aliases.append(\n",
    "    node_aliases[node_aliases.alias.str.contains(\"Lightning\")].sample(10, random_state=7))\n",
    "# add entirely random aliases\n",
    "tmp_aliases = tmp_aliases.append(node_aliases.sample(\n",
    "    10, random_state=7)).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "distance_measure = relative_lcs # tofix redefined below\n",
    "# the common substring needs to account for 70% of all letters of the longer string\n",
    "max_distance_threshold = 1 - 0.7\n",
    "\n",
    "distance_measure = relative_lcs  # jellyfish.levenshtein_distance\n",
    "max_distance_threshold = 0.46\n",
    "\n",
    "clusters, Z = cluster(tmp_aliases, distance_measure, max_distance_threshold)\n",
    "\n",
    "\n",
    "rcParams['font.family'] = 'cmr10'\n",
    "\n",
    "# plot dendrogram (only makes sense for smaller data...)\n",
    "fig = plt.figure(figsize=(9, 16))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "dn = scipy.cluster.hierarchy.dendrogram(\n",
    "    Z, orientation=\"right\", labels=clusters.alias.values, color_threshold=max_distance_threshold)\n",
    "ax.axvline(x=max_distance_threshold, color='r', ls=\"--\")\n",
    "ax.xaxis.set_ticks_position(\"top\")\n",
    "plt.savefig(\"alias_dendrogram_example.pdf\", papertype=\"letter\", bbox_inches=\"tight\")\n",
    "\n",
    "same_asn_clusters = get_same_asn_clusters(clusters, node_ips, whois_data)\n",
    "same_asn_clusters.groupby([\"cluster\", \"pub_key\"])[\"alias\"].agg(\n",
    "    lambda x: '|'.join(set(x))).reset_index()\n",
    "\n",
    "evaluate_single_result(clusters)\n",
    "tmp_aliases = node_aliases.copy()\n",
    "results = pd.concat([\n",
    "    evaluate_measure(tmp_aliases, relative_lcs, np.arange(0, 1, 0.01)),\n",
    "    evaluate_measure(tmp_aliases, lcs_distance, 1/np.arange(1, 15, 1)),\n",
    "    evaluate_measure(tmp_aliases, levenshtein_distance, np.arange(0, 10, 1)),\n",
    "    evaluate_measure(tmp_aliases, relative_levenshtein, np.arange(0, 1, 0.01)),\n",
    "    evaluate_measure(tmp_aliases, damerau_levenshtein_distance, np.arange(0, 10, 1)),\n",
    "    evaluate_measure(tmp_aliases, relative_damerau_levenshtein, np.arange(0, 1, 0.01)),\n",
    "    evaluate_measure(tmp_aliases, hamming_distance, np.arange(0, 10, 1)),\n",
    "    evaluate_measure(tmp_aliases, relative_hamming, np.arange(0, 1, 0.01)),\n",
    "    evaluate_measure(tmp_aliases, jaro_distance, np.arange(0, 1, 0.01)),\n",
    "    evaluate_measure(tmp_aliases, jaro_winkler_distance, np.arange(0, 1, 0.01))\n",
    "])\n",
    "\n",
    "results[results.cluster_count_min26 == 2].sort_values(\"node_count\", ascending=False).dropna()\n",
    "\n",
    "# plot method comparison\n",
    "bestMeasure_for_plot = results[results.groupby(['measure'])['cluster_count_min26'].transform(\n",
    "    max) == results['cluster_count_min26']].reset_index(drop=True)\n",
    "bestMeasure_for_plot = bestMeasure_for_plot.iloc[bestMeasure_for_plot.reset_index().groupby([\"measure\"])[\n",
    "    'node_count'].idxmax()]\n",
    "\n",
    "bestMeasure_for_plot = bestMeasure_for_plot.sort_values(\"node_count\", ascending=False)\n",
    "measure_list = bestMeasure_for_plot['measure'].values.tolist()\n",
    "measure_cat = pd.Categorical(bestMeasure_for_plot['measure'], categories=measure_list)\n",
    "bestMeasure_for_plot = bestMeasure_for_plot.assign(measure_cat=measure_cat)\n",
    "bestMeasure_for_plot[\"label_pos\"] = bestMeasure_for_plot[\"node_count\"]/2\n",
    "bestMeasure_for_plot[\"label_text\"] = bestMeasure_for_plot.apply(\n",
    "    lambda x: str(x[\"cluster_count\"]) + \" clusters at threshold \"+str(round(x['threshold'], 2)), axis=1)\n",
    "\n",
    "plot = ggplot(bestMeasure_for_plot) +\\\n",
    "    geom_bar(aes(x=\"measure_cat\", y=\"node_count\"), stat=\"identity\", colour=\"black\", fill=\"white\") +\\\n",
    "    geom_text(aes(x=\"measure_cat\", y=\"label_pos\", label=\"label_text\"), angle=90, colour=\"black\") +\\\n",
    "    labs(x=\"String distance measure\", y=\"Total clustered lightning nodes\") +\\\n",
    "    theme_publication +\\\n",
    "    theme(axis_text_x=element_text(rotation=45, vjust=1, hjust=1))\n",
    "\n",
    "plot.save(\"alias_clustering_white.pdf\", width=5, height=5)\n",
    "\n",
    "\n",
    "# print summary of best method:\n",
    "best = results[results.groupby(['measure'])['cluster_count_min26'].transform(\n",
    "    max) == results['cluster_count_min26']].reset_index(drop=True)\n",
    "best = best[best[\"node_count\"] == best[\"node_count\"].max()].iloc[0]\n",
    "print(best)\n",
    "\n",
    "print(\"Running final alias clustering with best configuration\")\n",
    "final_alias_cluster, _ = cluster(\n",
    "    node_aliases, globals()[best.measure.replace(\" \", \"_\")], best.threshold)\n",
    "#final_alias_cluster, _ = cluster(node_aliases.head(1000), relative_lcs, 0.46)\n",
    "alias_asn_clusters = get_same_asn_clusters(final_alias_cluster, node_ips, whois_data)[\n",
    "    [\"pub_key\", \"cluster\"]].drop_duplicates()\n",
    "alias_asn_clusters[\"cluster_origin\"] = \"alias/asn\"\n",
    "\n",
    "print(\"Clustering based on same IP/onion address\")\n",
    "same_ip_nodes = node_ips.drop(columns=['port']) \\\n",
    "    .groupby('ip_address') \\\n",
    "    .filter(lambda x: x['pub_key'].nunique() > 1) \\\n",
    "    .sort_values('ip_address')\n",
    "same_ip_nodes = same_ip_nodes.rename(columns={\"ip_address\": \"cluster\"})\n",
    "same_ip_nodes[\"cluster_origin\"] = \"address\"\n",
    "same_ip_nodes.head()\n",
    "\n",
    "print(\"Merging both clusterings\")\n",
    "combined = pd.concat([alias_asn_clusters, same_ip_nodes])\n",
    "G = nx.from_pandas_edgelist(combined, source=\"pub_key\", target=\"cluster\", create_using=nx.DiGraph)\n",
    "l = list(nx.weakly_connected_components(G))\n",
    "L = [dict.fromkeys(y, x) for x, y in enumerate(l)]\n",
    "d = {k: v for d in L for k, v in d.items()}\n",
    "mapping = pd.DataFrame(list(d.items()), columns=['pub_key', 'newcluster'])\n",
    "mapping = mapping.merge(combined, how=\"left\", on=\"pub_key\").drop(columns=\"cluster\").dropna()\n",
    "mapping = mapping.groupby([\"newcluster\", \"pub_key\"])[\"cluster_origin\"].agg(\n",
    "    lambda x: ' & '.join(set(x))).reset_index()\n",
    "mapping = mapping.merge(node_aliases, how=\"left\")\n",
    "\n",
    "final_clusters = mapping.groupby([\"newcluster\", \"pub_key\", \"cluster_origin\"])[\n",
    "    \"alias\"].agg(lambda x: ' | '.join(set(x.dropna()))).reset_index()\n",
    "final_clusters = final_clusters.rename(columns={\"newcluster\": \"cluster\"})\n",
    "final_clusters.to_csv(alias_address_clusters_csv_file, index=False)\n",
    "\n",
    "cluster_type_distribution = final_clusters.groupby(\"cluster_origin\")[\"pub_key\"].count()\n",
    "print(\"Best clustering algorithm:\\n\", cluster_type_distribution)\n",
    "print(\"Total nodes clustered: \", final_clusters[\"pub_key\"].count())\n",
    "print(\"Total cluster count:\", final_clusters.cluster.nunique())\n",
    "print(\"Largest clusters:\")\n",
    "final_clusters.groupby(\"cluster\").agg(\n",
    "    {\"pub_key\": len, \"alias\": lambda x: \"{%s}\" % ', '.join(x)}).sort_values(\"pub_key\", ascending=False).head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
